{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  <head>\n",
    "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
    "    <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n",
    "    <meta name=\"generator\" content=\"pandoc\" />\n",
    "    <title></title>\n",
    "    <style type=\"text/css\">code{white-space: pre;}</style>\n",
    "  </head>\n",
    "  \n",
    "\n",
    "# MACHINE LEARNING FOR RESEARCHERS\n",
    "\n",
    "# Notebook 1. Introduction to Machine Learning algorithms\n",
    "\n",
    "\n",
    "    \n",
    "This notebook introduces the most basic supervised machine learning methods, both in the context of <em>regression</em> and <em>classification</em>. \n",
    "\n",
    "In particular, the following contents are covered: \n",
    "\n",
    "<ol>\n",
    "    <li> Linear regression </li>\n",
    "    <li> Logistic regression </li>\n",
    "    <li> K-NN classifiers </li>\n",
    "</ol>\n",
    "\n",
    "It is highly recommended that this notebook is read and run after a first reading of the theory and in parallel with the slides available in AV. \n",
    "Note also that it is not required to develop any code. All examples are totally implemented, and therefore these notebooks have to be regarded as demonstrative material. The goal is understanding the operation of the algorithms. The notebook contains several questions that have to submitted through AV. \n",
    "\n",
    "The codes used for loading and plotting the Iris and MNIST data sets have been taken from <a href=https://github.com/ageron/handson-ml2>Geron (Github site)</a>. Please, consult the textbook for reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "Supervised learning models aimed at predicting an output which takes values in a continuous domain are called <b>regression</b> models. Some examples of these systems are stock price predictors, house price predictors, the progression of some disease after a given amount of time, etc. \n",
    "\n",
    "Linear regression models are those which can be expressed through hypothesis functions of the form:\n",
    "\n",
    "  \\begin{equation}\n",
    "  y(\\pmb{x}, \\pmb{w}) = w_0 + \\sum_{j=1}^{M-1} w_j \\phi_j(\\pmb{x}).\n",
    "  \\end{equation}\n",
    "\n",
    "Where the vector $\\pmb{x}$ is $D$-dimensional, the function basis $\\{\\phi_m\\}$ is $M-1$ dimensional, and \n",
    "  where $w_0$ is called the <em>bias</em> parameter, and $\\pmb{w}$ denotes the\n",
    "  vector $(w_0,\\ldots,w_{M-1})^T$. By assuming a dummy basis function\n",
    "  $\\phi_0(\\pmb{x})$ = 1, we get:\n",
    "\n",
    "  \\begin{equation}\n",
    "  y(\\pmb{x}, \\pmb{w}) = \\sum_{j=0}^{M-1} w_j \\phi_j(\\pmb{x}) = \\pmb{w}^T \\pmb{\\phi}(\\pmb{x})\n",
    "  \\end{equation}\n",
    "\n",
    "\n",
    "This models are called <em>linear</em> since the hypotheses are linear functions of the weights (even if the function basis are non-linear functions of $\\pmb{x}$).\n",
    "\n",
    "The loss function, which measure the error fitting the training set, is then given by the <b>squared error</b>:\n",
    "\n",
    "\\begin{equation}\n",
    "  J(\\pmb{w}) = \\frac{1}{2} \\sum_{n=1}^N (\\pmb{w}^T \\pmb{\\phi}(\\pmb{x}_n)- t_n)^2\n",
    "  \\end{equation}\n",
    "\n",
    "We will consider first the simplest regression case with a single-dimensional input space and a single target. A toy data set, similar to the one used in the slides is created and shown next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries required\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Required to use matplotlib inside the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# We set a fixed seed to get always the same results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Data set generation\n",
    "X = np.linspace(0,1,num=10)\n",
    "X = np.array([X]).T # Since X is single-dimensional, we have to convert it to a matrix to transpose it \n",
    "t = np.sin(2*np.pi*X) + 0.1*np.random.randn(10, 1)\n",
    "xfull = np.linspace(0,1,num=100);\n",
    "xfull = np.array([xfull]).T\n",
    "treal = np.sin(2*np.pi*xfull);\n",
    "\n",
    "# Data set plotting\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$t$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial feature space\n",
    "\n",
    "The simplest approximation is working directly in the input space, that is, with the basis $\\{\\phi_m\\}$ = $\\{1, x\\}$ (note that $\\pmb{x}$ in this case is single-dimensional thus is simply written as $x$). \n",
    "\n",
    "Finding the best weights ($\\pmb{w}$) fitting the data set requires minimizing the loss function by solving the normal equations:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\pmb{w}^* = (\\pmb{\\Phi}^T \\pmb{\\Phi})^{-1} \\pmb{\\Phi}^T \\pmb{t}\n",
    "\\end{equation}\n",
    "\n",
    "where the design matrix $\\pmb{\\Phi}$ is:\n",
    "\n",
    "  \\begin{equation}\n",
    "  \\pmb{\\Phi} = \\begin{bmatrix}\n",
    "  \\phi_0(\\pmb{x}_1) & \\phi_1(\\pmb{x}_1) & \\dots & \\phi_{M-1}(\\pmb{x}_1) \\\\\n",
    "  \\phi_0(\\pmb{x}_2) & \\phi_1(\\pmb{x}_2) & \\dots & \\phi_{M-1}(\\pmb{x}_2) \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\phi_0(\\pmb{x}_N) & \\phi_1(\\pmb{x}_N) & \\dots & \\phi_{M-1}(\\pmb{x}_N) \\\\\n",
    "  \\end{bmatrix} =\\begin{bmatrix}\n",
    "  1 & x_1  \\\\\n",
    "1 & x_2  \\\\\n",
    "  \\vdots & \\vdots \\\\\n",
    "1 & x_N  \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\end{equation}\n",
    "  \n",
    "  Next functions find the best solution giving the design matrix and predict the target of new points (given in the feature space $\\pmb{\\phi}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(Phi,t):\n",
    "    w = np.linalg.inv(Phi.T.dot(Phi)).dot(Phi.T).dot(t)\n",
    "    return w\n",
    "\n",
    "def predict(w, phi):\n",
    "    y = phi.dot(w)  # When Φ contains several points (rows) to predict, \n",
    "                    # instead of evaluating it one by one as in eq. (9) of the unit 1 slides \n",
    "                    # it is more simple to compute in matrix form as y=Φw\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the design matrix and find the best fitting for the toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.c_[np.ones((X.shape[0], 1)), X] # Each point x is transformed to feature space (1,x)\n",
    "wopt = fit(Phi, t)\n",
    "\n",
    "xnew = np.arange(start=0.0,stop=1.0,step=0.01)\n",
    "phinew = np.c_[np.ones((xnew.shape[0], 1)), xnew]  # Each point x is transformed to feature space (1,x)\n",
    "y = predict(wopt, phinew)\n",
    "\n",
    "plt.plot(xnew, y, \"r-\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial feature space\n",
    "\n",
    "What do you think about previous result? \n",
    "\n",
    "Actually it seems a pretty limited approximation. The cause is the lack of flexibility in the model (<b>it can only produce lines!</b>). Of course, the solution is to work with a more expressive feature space. Let's  try for example the feature space compose by polynomials up to $(M-1)$-degree, that is, $\\{\\pmb{\\phi}_m\\}$ = $\\{1,x,x^2,\\ldots,x^{M-1}\\}$. So, the design matrix is now:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pmb{\\Phi} = \\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 & \\ldots & x_1^{M-1} \\\\\n",
    "1 & x_2 & x_2^2 & \\ldots & x_2^{M-1}  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_N & x_N^2 & \\ldots & x_N^{M-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The normalequation and predict functions do not change, only the transformation the feature space. \n",
    "\n",
    "Next code show the result up to M=9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 9\n",
    "Phi = np.c_[np.ones((X.shape[0], 1))]\n",
    "xnew = np.arange(start=0.0,stop=1.0,step=0.01)\n",
    "phinew = np.ones((xnew.shape[0],1))\n",
    "fig, ax = plt.subplots(nrows=M, ncols=1, figsize=(5,15))\n",
    "fig.tight_layout()\n",
    "\n",
    "for m in range(1,10):\n",
    "    Phi = np.c_[Phi,X**m] # Add column X^M \n",
    "    wopt = fit(Phi, t)\n",
    "    \n",
    "    phinew = np.c_[phinew, xnew**m]  # Add column xnew^M\n",
    "    y = predict(wopt, phinew)\n",
    "\n",
    "    ax[m-1].set_ylim(-1.5,1.5)\n",
    "    ax[m-1].set_title(f'M={m}')\n",
    "    ax[m-1].plot(xfull, treal, \"g-\")\n",
    "    ax[m-1].plot(xnew, y, \"r-\")\n",
    "    ax[m-1].plot(X, t, \"b.\")\n",
    "    ax[m-1].axis([0, 1, -1.5, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the quality of the result depends on the model complexity (M). As stated before, inflexible models underfit the data (e.g., M=1,2). On the other hand, too flexible models overfit the data (e.g. M=8, 9). Neither underfit nor underfit models generalize correctly to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "In order to control automatically the complexity of the model a regularization term is introduced in the loss function: \n",
    "\n",
    "\\begin{equation}\n",
    "  \\widetilde{J}(\\pmb{w}) = \\frac{1}{2} \\sum_{n=1}^N [\\sum_{j=0}^{M-1} w_j \\phi_j(\\pmb{x}_n) - t_n]^2 + \\frac{\\lambda}{2}\\sum_{j  =0}^{M-1}|w_j|^q\n",
    "  \\label{LOSSREG}\n",
    "  \\end{equation}\n",
    "\n",
    "the term $\\lambda$ is the regularization hyper-parameter, and $q$ determines the type of regularization. The so-called ridge-regression uses $q$=2, and has the advantage of having a closed\n",
    "  form version of the normal equations:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\pmb{w}^* = (\\lambda I + \\pmb{\\Phi}^T \\pmb{\\Phi})^{-1} \\pmb{\\Phi}^T \\pmb{t}\n",
    "  \\label{WOPTREG}\n",
    "  \\end{equation}\n",
    "  \n",
    "  Let's create a new function implementing this approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitridge(L,Phi,t):\n",
    "    w = np.linalg.inv(L*np.eye(Phi.shape[1])+Phi.T.dot(Phi)).dot(Phi.T).dot(t)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run it for different values of $\\lambda$ (L in the function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the Phi and phinew are still created from the previous code block for M=9\n",
    "LAMBDAS = 7\n",
    "fig, ax = plt.subplots(nrows=LAMBDAS, ncols=1, figsize=(5,15))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(LAMBDAS):\n",
    "    lnL = -i*3;\n",
    "    wopt = fitridge(np.exp(lnL),Phi, t)\n",
    "    y = predict(wopt, phinew)\n",
    "\n",
    "    ax[i].set_ylim(-1.5,1.5)\n",
    "    ax[i].set_title(f'ln $\\lambda$={lnL}')\n",
    "    ax[i].plot(xfull, treal, \"g-\")\n",
    "    ax[i].plot(xnew, y, \"r-\")\n",
    "    ax[i].plot(X, t, \"b.\")\n",
    "    ax[i].axis([0, 1, -1.5, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "\n",
    "To determine the optimal $\\lambda$ hyper-parameter, cross-validation can be used. Cross-validation consist of separating a set of points from the training set to check on them the accuracy of the prediction (let us remark that these points can't be used for training). K-fold cross-validation repeats this procedure with K independently-drawn validation sets, and averages the results. The best hyper-parameters are those maximizing the <b>average</b> performance. \n",
    "\n",
    "\n",
    "Next code performs this operation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "\n",
    "LAMBDAS = 20\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True) # Define the split - into 5 folds \n",
    "kf.get_n_splits(Phi) # returns the number of splitting iterations in the cross-validator\n",
    "loss = np.zeros([LAMBDAS])\n",
    "\n",
    "for  i in range(LAMBDAS):\n",
    "    lnL = -i\n",
    "\n",
    "    for train_index, validation_index in kf.split(Phi):\n",
    "        # print(train_index, validation_index)\n",
    "        Phi_train, Phi_validation = Phi[train_index], Phi[validation_index]\n",
    "        t_train, t_validation = t[train_index], t[validation_index]\n",
    "    \n",
    "        wopt = fitridge(np.exp(lnL),Phi_train, t_train)\n",
    "        y = predict(wopt, Phi_validation)\n",
    "        loss[i] += (t_validation - y).T.dot(t_validation - y)[0]\n",
    "        \n",
    "# Loss plotting versus -ln $\\lambda$\n",
    "plt.plot(np.arange(0,LAMBDAS), np.log(loss/K), \"*-\")\n",
    "plt.xlabel(\"$- \\ln\\;\\lambda$\", fontsize=18)\n",
    "plt.ylabel(\"$\\ln\\;\\overline{loss}$\", rotation=90, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows a minimal loss for a regularization hyper-parameter $-\\ln \\lambda \\sim 10-12$. Note that the method is subject to randomness, since the folding of the sets is random. Therefore, different executions yield slightly different results. Due to that effect, it is recommended that the final prediction loss estimate is performed on a separate test set, separated from the training and the validation sets <b>before</b> the cross-validation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting using gradient descent optimization\n",
    "\n",
    "Solving the normal equations can be demanding in terms of CPU power when the number of instances in the training set is large. An alternative method, valid also when closed-form solutions are not available, is the use of gradient descent. It is an iterative method that departing from some randomly chosen point, moves it at each iteration in the contrary (since the goal is seeking for the minimum) to the maximal slope increase direction, that is, towards $-\\nabla J(\\pmb{w})$. Summarizing:\n",
    "\n",
    "<ol>\n",
    "<li>\n",
    "  Set initial weights for $\\pmb{w}$ (e.g., randomly)\n",
    "<li> Repeat:\n",
    "  \\begin{equation*}\n",
    "  \\pmb{w}^{(\\text{new})} = \\pmb{w}^{(\\text{old})} - \\eta \\nabla J(\\pmb{w}^{(\\text{old})}) = \\pmb{w}^{(\\text{old})} + \\eta \\pmb{\\Phi}^T [\\pmb{t} - \\pmb{\\Phi} \\pmb{w}^{(\\text{old})}]\n",
    "  \\end{equation*}\n",
    "  until convergence.\n",
    "</ol>\n",
    "\n",
    "$\\eta$ is called the learning rate and trade-offs the convergence and speed of the algorithm.\n",
    "\n",
    "Let's implement a fit function based on gradient descent and try it on the previous example for different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(Phi,t,w):\n",
    "    m = X.shape[0]\n",
    "    gradients = -Phi.T.dot(t - Phi.dot(w))\n",
    "    return gradients\n",
    "\n",
    "def fitGD(Phi,t,eta):\n",
    "    n_iterations = 1000\n",
    "    w = np.random.randn(Phi.shape[1],1) \n",
    "    for iteration in range(n_iterations):\n",
    "        w = w - eta * gradient(Phi,t,w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "M = 9\n",
    "valuesETAS = [0.001, 0.01, 0.1, 1]\n",
    "ETAS = len(valuesETAS)\n",
    "Phi = np.c_[np.ones((X.shape[0], 1))]\n",
    "xnew = np.arange(start=0.0,stop=1.0,step=0.01)\n",
    "phinew = np.ones((xnew.shape[0],1))\n",
    "fig, ax = plt.subplots(nrows=M, ncols=ETAS, figsize=(10,15))\n",
    "fig.tight_layout()\n",
    "\n",
    "for m in range(1,M+1):\n",
    "    Phi = np.c_[Phi,X**m] # Add column X^M \n",
    "    phinew = np.c_[phinew, xnew**m]  # Add column xnew^M\n",
    "        \n",
    "    for e in range(ETAS):\n",
    "        eta = valuesETAS[e]\n",
    "        wopt = fitGD(Phi, t, eta)\n",
    "        y = predict(wopt, phinew)\n",
    "\n",
    "        ax[m-1,e].set_ylim(-1.5,1.5)\n",
    "        ax[m-1,e].set_title(f'M={m}, $\\eta$={eta}')\n",
    "        ax[m-1,e].plot(xfull, treal, \"g-\")\n",
    "        ax[m-1,e].plot(xnew, y, \"r-\")\n",
    "        ax[m-1,e].plot(X, t, \"b.\")\n",
    "        ax[m-1,e].axis([0, 1, -1.5, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small learning rates converge slower, so they require more iterations and more time. On the other hand, large learning rates are quicker but may lead to non-convergence situations (as shown in the right-most figure in the previous plot). We can do an experiment with random data to measure the time differences between both fitting methods (normal equations and gradient descent) for different sizes of the training set: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if we have more data?\n",
    "\n",
    "Overfitting can be avoided if more training data is available. As an example, we can repeat the first experiment for a slightly larger data set of $N$=20 points. As can be seen in the next figures a model with moderate complexity does not suffer from overfitting, like before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set generation\n",
    "X = np.linspace(0,1,num=20)\n",
    "X = np.array([X]).T # Since X is single-dimensional, we have to convert it to a matrix to transpose it \n",
    "t = np.sin(2*np.pi*X) + 0.1*np.random.randn(20, 1)\n",
    "xfull = np.linspace(0,1,num=100);\n",
    "xfull = np.array([xfull]).T\n",
    "treal = np.sin(2*np.pi*xfull);\n",
    "\n",
    "M = 9\n",
    "Phi = np.c_[np.ones((X.shape[0], 1))]\n",
    "xnew = np.arange(start=0.0,stop=1.0,step=0.01)\n",
    "phinew = np.ones((xnew.shape[0],1))\n",
    "fig, ax = plt.subplots(nrows=M, ncols=1, figsize=(5,15))\n",
    "fig.tight_layout()\n",
    "\n",
    "for m in range(1,10):\n",
    "    Phi = np.c_[Phi,X**m] # Add column X^M \n",
    "    wopt = fit(Phi, t)\n",
    "    \n",
    "    phinew = np.c_[phinew, xnew**m]  # Add column xnew^M\n",
    "    y = predict(wopt, phinew)\n",
    "\n",
    "    ax[m-1].set_ylim(-1.5,1.5)\n",
    "    ax[m-1].set_title(f'M={m}')\n",
    "    ax[m-1].plot(xfull, treal, \"g-\")\n",
    "    ax[m-1].plot(xnew, y, \"r-\")\n",
    "    ax[m-1].plot(X, t, \"b.\")\n",
    "    ax[m-1].axis([0, 1, -1.5, 1.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with sklearn\n",
    "\n",
    "Next, we will discuss how to work directly with scikit-learn (or <b>sklearn</b> in short) library (the python library for machine learning). Let's start with a linear regression with trivial feature space, as done earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, t)\n",
    "y = lin_reg.predict(xfull) \n",
    "\n",
    "plt.plot(xnew, y, \"r-\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask sklearn to use a feature space, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=True) # Degree M=2 with bias term\n",
    "Phi = poly_features.fit_transform(X)\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(Phi, t)\n",
    "phifull = poly_features.transform(xfull)\n",
    "y = lin_reg.predict(phifull)\n",
    "\n",
    "plt.plot(xfull, y, \"r-\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of options for Linear Regression can be obtained by executing the next command (in general you get help for any python command or function this way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the documentation, predictors offer many convenient methods, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The fitting accuracy in the training set is {lin_reg.score(Phi,t)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, regularizers like Ridge, Lasso or ElasticNet are at hand: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=20, include_bias=True) # Degree M=20 with bias term\n",
    "Phi = poly_features.fit_transform(X)\n",
    "lin_reg = LinearRegression() # The regularization parameter is called $\\alpha$ in scikit-learn\n",
    "lin_reg_other = Lasso(alpha=np.exp(-10)) # The regularization parameter is called $\\alpha$ in scikit-learn\n",
    "lin_reg.fit(Phi, t)\n",
    "lin_reg_other.fit(Phi, t)\n",
    "\n",
    "phifull = poly_features.transform(xfull)\n",
    "y = lin_reg.predict(phifull)\n",
    "y_other = lin_reg_other.predict(phifull)\n",
    "\n",
    "plt.plot(xfull, y_other, \"b-\") # Lasso in blue\n",
    "plt.plot(xfull, y, \"r-\") # Unregularized linear regression in red \n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we develop a full and realistic Regression example, where we show how to use also other characteristics, like cross-validation, directly from python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 1\n",
    "> **Propose some alternative feature space for the previous data set. For example, one alternative feature space could be {1,x,exp(x),exp(x^2)}. Make use of the fact that the data are generated from a sin function**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A full regression example: The California House Value Data Set\n",
    "\n",
    "In this section we provide a full example on a realistic data set aimed at predicting the median house values in Californian districts, given a number of features from these districts. This example is developed in Geron's chapter 2.\n",
    "\n",
    "First, the data set is fetched and some rows of the data set are shown as an example. \n",
    "\n",
    "<b>Note 1:</b> The data set is handled using Pandas. Pandas is a library of Python for data set manipulation. Interested students can consult the book \"Python Data Science Handbook\" by Jake VanderPlas, which provides a clear and thorough description of this library (and others), and is available in AV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's show the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for model fitting\n",
    "\n",
    "Next, the raw table is prepared for its use in the regression model. For that the following steps are done:\n",
    "\n",
    "<ol>\n",
    "    <li> Computation of representative features (e.g. rooms_per_household)\n",
    "    <li> Separation of targets and features\n",
    "    <li> Substitution of missing features (e.g. number of rooms) for the median value of the column\n",
    "    <li> Substitution of categorical variables (e.g. ocean_proximity) for a 1-K encoding\n",
    "    <li> Separation of training and test sets\n",
    "</ol>\n",
    "\n",
    "You can skip most of this step since it involves dataframes and ndarrays manipulation. The significant part is that at the end result is divided in training and testing sets, and targets are provided in separated vectors as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "housing = load_housing_data() # Since we perform destructive change, we reload the data\n",
    "targets = housing[\"median_house_value\"].copy()\n",
    "housing = housing.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "train_set, test_set, targets_train, targets_test = train_test_split(housing_prepared, targets, test_size=0.2) # Reserve 20% of instances for testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model with Ridge regularization and Cross-Validation\n",
    "\n",
    "Finally, a regression can be easily implemented with all of the functionality discussed earlier, by using the sklearn modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = train_set\n",
    "X_test = test_set\n",
    "t = targets_train\n",
    "t_test = targets_test\n",
    "\n",
    "# Now using Ridge with cross-validation\n",
    "alphas = np.exp(-np.arange(start=1,stop=5,step=.1)).tolist()\n",
    "ridge_reg = RidgeCV(alphas=alphas, \n",
    "                    cv=10, \n",
    "                    store_cv_values=False, \n",
    "                    normalize=True, \n",
    "                    fit_intercept=True)\n",
    "ridge_reg.fit(X, t)\n",
    "y = ridge_reg.predict(X)\n",
    "y_test = ridge_reg.predict(X_test)\n",
    "rmse_train = np.sqrt(mean_squared_error(t, y))\n",
    "rmse_test = np.sqrt(mean_squared_error(t_test, y_test))\n",
    "\n",
    "print(f'Ridge regression achieved minimum MSE for regularizer term {ridge_reg.alpha_:.05f}')\n",
    "print(f'The score (rmse) in the training set is {rmse_train:.00f}, with accuracy of {ridge_reg.score(X,t)*100:.02f}%')\n",
    "print(f'The score (rmse) in the test set is {rmse_test:.00f}, with accuracy of {ridge_reg.score(X_test,t_test)*100:.02f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 2\n",
    "> **Try to explain why the results got for the last example are rather poor, and propose some suitable idea to improve the results using only linear regression and linear classification**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "A classifier tries to predict an output belonging to a finite set of values. For example, a classifier may have the purpose of indicating whether or not a patient suffers from a pathology based on various input data or features. The target in this example is binary (YES/NO). As another example, for the MNIST dataset the goal can be  predicting the number that appears in an image (0/.../9). In a musical classification application the goal can be determining to which genre a song belongs (ROCK/POP/CLASSIC/...), etc.\n",
    "\n",
    "More formally, in <b>classification</b> problems the goal is to take a $D$-dimensional input vector $\\pmb{x}$ and assign it to one of $K$ classes $\\mathcal{C}_k$ for $k$=$1,\\ldots,K$. As in the regression case we assume an input transformation to a <b>feature space</b> using a set of fixed (non-linear) $M-1$ basis functions $\\{\\phi_m(\\pmb{x})\\}$, for $m$=$1,\\ldots,M-1$ and the dummy function $\\phi_0(\\pmb{x})$ = 1.\n",
    "\n",
    "Thus, in matrix notation, $\\pmb{\\phi}$ = $(\\phi_0(\\pmb{x}),\\ldots,\\phi_{M-1}(\\pmb{x}))^T$.\n",
    "\n",
    "The regions assigned to different classes are separated by <b>decision boundaries</b>. The term <b>linear classification</b> is used when these decision boundaries (in the feature space) are hyperplanes. If a data set can be separated without misclassification errors by some hyperplane, then it is called <b>linearly separable</b>.\n",
    "\n",
    "Logistic regression is a linear binary classification model which models the posterior probability of $\\mathcal{C}_1$ using the <b>logistic sigmoid</b>:\n",
    "  \n",
    "\\begin{equation}\n",
    "y(\\pmb{w}, \\pmb{x}) = p(\\mathcal{C}_1|\\pmb{\\phi}) = \\sigma(\\pmb{w}^T\\pmb{\\phi}) = \\frac{1}{1+e^{-\\pmb{w}^T\\pmb{\\phi}}}\n",
    "\\end{equation}\n",
    "\n",
    "and the probability of $\\mathcal{C}_2$ is the complementary:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathcal{C}_2|\\pmb{\\phi}) = 1- p(\\mathcal{C}_1|\\pmb{\\phi}) \n",
    "\\end{equation}\n",
    "\n",
    "Let's have a look to the shape of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid plot\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sig = sigmoid(z)\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(z, sig, \"b-\", linewidth=2, label=r\"$\\sigma(a) = \\frac{1}{1 + e^{-a}}$\")\n",
    "plt.xlabel(\"a\")\n",
    "plt.legend(loc=\"upper left\", fontsize=18)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final prediction is $\\mathcal{C}_1$ when $y(\\pmb{w}, \\pmb{x}) \\ge 0.5$, and $\\mathcal{C}_2$ otherwise.\n",
    "\n",
    "Given a labeled data set $\\pmb{X}$=$\\{\\pmb{x}_1,\\ldots,\\pmb{x}_N\\}$, $\\pmb{t}$ = $\\{t_1,\\ldots,t_N\\}$, where $t_n$ = 1 if $\\pmb{x}_1 \\in \\mathcal{C}_1$, or 0 otherwise. The <b>cross-entropy</b> loss function is given by: \n",
    "\n",
    "\\begin{equation}\n",
    "J(\\pmb{w}) = - \\ln p(\\pmb{t}|\\pmb{w}) = - \\sum_{n=1}^N [t_n \\ln y_n + (1-t_n)\\ln(1-y_n)]\n",
    "\\end{equation}\n",
    "\n",
    "This function is concave, thus with unique minimum, which can be found using the iterative Newton-Raphson method:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{w}^\\text{(new)} = \\pmb{w}^\\text{(old)} - (\\pmb{\\Phi}^T \\pmb{R} \\pmb{\\Phi})^{-1} \\pmb{\\Phi}^T (\\pmb{y} - \\pmb{t})\n",
    "\\end{equation*}\n",
    "\n",
    "with $\\pmb{R}$ a $N\\times N$ diagonal matrix such that element $(\\pmb{R})_{nn}$ = $y_n (1-y_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris data set\n",
    "\n",
    "We are going to see how this method applies to an example data set known as the <b>Iris data set</b>, which contains features about flower petals and sepals labeled with the exact species to which they belong. Our goal is to distinguish copies of the <a href=https://www.wikiwand.com/en/Iris_virginica>Iris species from Virginia</a>, compared to two other species of the Iris type. \n",
    "\n",
    "This data set is available with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "# print(iris.DESCR) # Uncomment to show infor about the data set\n",
    "\n",
    "# Let's create the feature and the target set\n",
    "X = iris[\"data\"][:, (2, 3)]  # Petal length and width\n",
    "t = (iris[\"target\"] == 2).astype(np.int)\n",
    "t = t.reshape([X.shape[0],1])\n",
    "\n",
    "# Since only two features are used, we can plot the data set\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "\n",
    "plt.text(3.5, 1.5, \"Non Iris-Virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris-Virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create <b>fit</b> and <b>predict</b> functions like the case of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(Phi,t): \n",
    "    n_iterations = 1000\n",
    "    w = np.random.randn(Phi.shape[1],1) # Initial w at random\n",
    "     \n",
    "    for iteration in range(n_iterations):\n",
    "        y = sigmoid(Phi.dot(w))\n",
    "        R = np.diag(np.maximum(0.00001, np.multiply(y,1-y)).reshape([Phi.shape[0],])) # convert to array from matrix !\n",
    "        w = w - np.linalg.inv(Phi.T.dot(R.dot(Phi))).dot(Phi.T).dot(y-t)\n",
    "    \n",
    "    return w\n",
    "    \n",
    "def predict(phi, w):\n",
    "    h = (sigmoid(phi.dot(w))>0.5).astype(int)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use them to find a linear separation boundary and plot it (note that the $\\mathcal{C}_1$ probabilities are indicated as a contour plot in the resulting figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set a fixed seed to get always the same results\n",
    "np.random.seed(23)\n",
    "\n",
    "Phi = np.c_[np.ones([X.shape[0],1]), X]  # features: {1,x1,x2}\n",
    "wopt = fit(Phi,t)\n",
    "y = predict(Phi, wopt) \n",
    "\n",
    "accuracy = np.mean(y == t.reshape(t.shape[0],1))\n",
    "print(f'Accuracy of {accuracy*100:.02f}%') \n",
    "\n",
    "# Plot the decision boundary\n",
    "x0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1), np.linspace(0.8, 2.7, 200).reshape(-1, 1),)\n",
    "xfull = np.c_[x0.ravel(), x1.ravel()]\n",
    "phifull = np.c_[np.ones([xfull.shape[0],1]), xfull] \n",
    "yfull = sigmoid(phifull.dot(wopt))\n",
    "\n",
    "zz = yfull.reshape(x0.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(wopt[1] * left_right + wopt[0]) / wopt[2]\n",
    "\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spherical vs. Torus data set\n",
    "\n",
    "In this second example, we generate an artificial data set which has two classes with an spherical and torus-like distributions, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "r = np.random.rand(50, 1) #0...1\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_0 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "r = 0.9 + np.random.rand(50, 1) #0.9...1.9\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_1 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "X = np.r_[X_0, X_1]\n",
    "t = np.r_[np.zeros([50,1]),np.ones([50,1])]\n",
    "\n",
    "# Plot data set\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the input space is not linearly separable, but it is easy to construct a feature space (with 2-degree polynomials) that it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=True) # Degree M=2 with bias term\n",
    "Phi = poly_features.fit_transform(X)\n",
    "\n",
    "wopt = fit(Phi,t)\n",
    "y = predict(Phi, wopt) \n",
    "\n",
    "accuracy = np.mean(y == t.reshape(t.shape[0],1))\n",
    "print(f'Accuracy of {accuracy*100:.02f}%') \n",
    "\n",
    "# Plot the decision boundary\n",
    "x0, x1 = np.meshgrid(np.linspace(-2, 2, 500).reshape(-1, 1), np.linspace(-2, 2, 500).reshape(-1, 1),)\n",
    "xfull = np.c_[x0.ravel(), x1.ravel()]\n",
    "phifull = poly_features.transform(xfull)\n",
    "yfull = sigmoid(phifull.dot(wopt))\n",
    "\n",
    "zz = yfull.reshape(x0.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(wopt[1] * left_right + wopt[0]) / wopt[2]\n",
    "\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.tab10, levels=[0.5])\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the <b>boundary in the feature space is a hyperplane</b>, though in the input space hasn't linear shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 3\n",
    "> **Propose some feature space suitable for the data set created in the next example. Note that the condition for a point to be in one class or another is:**\n",
    "\\begin{equation}\n",
    "t = \\begin{cases}\n",
    "\\mathcal{C}_1\\;\\;\\text{if $x>\\sqrt{y}$ and $x<y^2$}\\\\\n",
    "\\mathcal{C}_2\\;\\;\\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "x = 5*np.random.rand(100, 1) \n",
    "y = 5*np.random.rand(100, 1) \n",
    "\n",
    "t = np.logical_and(X[:,0]>X[:,1]**(1/2),X[:,0]<X[:,1]**(2))\n",
    "\n",
    "# Plot data set\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "\n",
    "plt.axis([0, 5, 0, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A full logistic regression example: The MNIST Data Set\n",
    "\n",
    "In this section we explore a more complex problem using the MNIST data set, and using the sklearn libraries: \n",
    "\n",
    "<ol>\n",
    "<li> We consider a multi-class problem using the soft-max loss function\n",
    "<li> We introduce regularization (and also cross-validation to determine $\\lambda$)\n",
    "<li> A test set is separated from the data set to asses the accuracy of results\n",
    "<li> Other performance measurements are computed using the confusion matrix\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST \n",
    "\n",
    "First, we download (it may take some time) and show some examples of the MNIST data set which was described in Unit 1. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "# print(mnist.DESCR) # Uncomment to show description\n",
    "X, t = mnist[\"data\"], mnist[\"target\"] # N = 70000, D = 784 \n",
    "X_train, X_test, t_train, t_test = X[:60000], X[60000:], t[:60000], t[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.plasma, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do softmax classification using sklearn (it takes some seconds) and show some statistics and tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Solver is lbfgs which uses Ridge regularization by default\n",
    "clf = LogisticRegression(random_state=0, multi_class='multinomial').fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The score (accurary) in the training set is {clf.score(X_train,t_train)*100:.02f}%')\n",
    "print(f'The score (accuracy) in the test set is {clf.score(X_test,t_test)*100:.02f}%\\n')\n",
    "\n",
    "plot_digits(X_test[:10], images_per_row=10)\n",
    "print('Labels predicted for the 10 first pictures in the test set')\n",
    "print(clf.predict(X_test[:10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = clf.predict(X_test)\n",
    "conf_mx = confusion_matrix(t_test, y_test)\n",
    "print(conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite using the original input space this classifier works quite well!!\n",
    "\n",
    "Let's try to tell sklearn to use cross-validation to determine the best regularization parameter:\n",
    "\n",
    "<b style=\"color:red\"> This fitting requires a high-end computer (about 2-3 minutes in a machine with 12 cores and 64G of RAM). Let enough time for running the cell. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Solver is lbfgs which uses Ridge regularization by default\n",
    "clfCV = LogisticRegressionCV(cv=4, random_state=0, multi_class='multinomial').fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The score (accurary) in the training set is {clfCV.score(X_train,t_train)*100:.02f}%')\n",
    "print(f'The score (accuracy) in the test set is {clfCV.score(X_test,t_test)*100:.02f}%\\n')\n",
    "\n",
    "plot_digits(X_test[:10], images_per_row=10)\n",
    "print('Labels predicted for the 10 first pictures in the test set')\n",
    "print(clfCV.predict(X_test[:10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is similar to the previous case! Thus, not bad for a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 4\n",
    "> **Given the previous confusion matrix, determine the precision and recall for the class \"6\"**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-Nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-Nearest neighbors is a nonlinear classification method which assigns a point $\\pmb{x}$ to the class with the majority vote among its $k$-nearest neighbors. Different distances yield particular properties, and some of the most common are Euclidean, Cosine, Manhattan, etc. \n",
    "\n",
    "Its use with sklearn is straightforward. We are going to apply it to the Spherical vs Torus data set and to the MNIST data set, and check how it compares to the logistic/softmax regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spherical vs Torus data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "r = np.random.rand(500, 1) #0...1\n",
    "a = 2*np.pi*np.random.rand(500,1) #0..2pi\n",
    "X_0 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "r = 0.9 + np.random.rand(500, 1) #0.9...1.9\n",
    "a = 2*np.pi*np.random.rand(500,1) #0..2pi\n",
    "X_1 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "testsize = int(X_0.shape[0]*0.1)\n",
    "X_0_train, X_0_test = X_0[testsize:], X_0[:testsize]\n",
    "X_1_train, X_1_test = X_1[testsize:], X_1[:testsize]\n",
    "X_test = np.r_[X_0_test, X_1_test]\n",
    "X_train = np.r_[X_0_train, X_1_train]\n",
    "t_test = np.r_[np.zeros([testsize,1]), np.ones([testsize,1])]\n",
    "t_train = np.r_[np.zeros([500-testsize,1]), np.ones([500-testsize,1])]\n",
    "\n",
    "# Plot test data set\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(X_test[t_test.reshape(X_test.shape[0],)==0, 0], X_test[t_test.reshape(X_test.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X_test[t_test.reshape(X_test.shape[0],)==1, 0], X_test[t_test.reshape(X_test.shape[0],)==1, 1], \"g^\")\n",
    "\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(metric='haversine', n_neighbors=5)\n",
    "neigh.fit(X_train, t_train)\n",
    "print(f'Accuracy of {neigh.score(X_test,t_test)*100:.02f}%') \n",
    "print(neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy seems similar to that obtained with the logistic regression, but there are a two hyper-parameters (metric and n_neighbors) that have been just hard-coded. We can do an exhaustive search and find the best ones using cross-validation very easily with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "parameters = {'metric':['euclidean', 'minkowski', 'chebyshev', 'haversine'], 'p':range(1,10), 'n_neighbors':range(1,15)}\n",
    "clfaux = KNeighborsClassifier()\n",
    "clf = GridSearchCV(estimator=clfaux, param_grid=parameters, cv=8, scoring='precision_macro')\n",
    "clf.fit(X_train,t_train)\n",
    "print(clf.best_params_)\n",
    "print(f'Accuracy of {clf.score(X_test,t_test)*100:.02f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data set\n",
    "\n",
    "<b style=\"color:red\"> Despite using only about 5% of the data set instances, this fitting requires a high-end computer (about 2-3 minutes in a machine with 12 cores and 64G of RAM). Let enough time for running the cell. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "# print(mnist.DESCR) # Uncomment to show description\n",
    "X, t = mnist[\"data\"], mnist[\"target\"] # N = 70000, D = 784 \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, t = shuffle(X, t, random_state=0)\n",
    "\n",
    "X_train, X_test, t_train, t_test = X[:1000], X[1000:1200], t[:1000], t[1000:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Search best hyper-parameters\n",
    "parameters = {'metric':['sokalmichener', 'jaccard', 'matching', 'dice'], 'n_neighbors':range(1,15)}\n",
    "clfaux = KNeighborsClassifier()\n",
    "clf = GridSearchCV(estimator=clfaux, param_grid=parameters, cv=8, scoring='precision_macro')\n",
    "clf.fit(X_train,t_train)\n",
    "print(clf.best_params_)\n",
    "\n",
    "print(f'The score (accurary) in the training set is {clf.score(X_train,t_train)*100:.02f}%')\n",
    "print(f'The score (accuracy) in the test set is {clf.score(X_test,t_test)*100:.02f}%\\n')\n",
    "\n",
    "plot_digits(X_test[:10], images_per_row=10)\n",
    "print('Labels predicted for the 10 first pictures in the test set')\n",
    "print(clf.predict(X_test[:10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even using 3% of the instances, the accuracy is similar to that of the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 5\n",
    "> **With MNIST we haven't used a feature space, but we worked directly in the input space. It could be of interest (as we will see in Unit 3) to make use of a smaller dimensional spaces. In this question you are asked to propose some features that can be used to simplify the problem. For example, one feature can be the ratio of yellow pixels to the total number of pixels in the instance. Try to propose some additional features for this problem.**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1163px",
    "left": "1185.6px",
    "top": "111.133px",
    "width": "344.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
