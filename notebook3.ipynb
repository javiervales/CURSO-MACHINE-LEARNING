{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING FOR RESEARCHERS\n",
    "\n",
    "# Notebook 3. Unsupervised learning methods\n",
    "\n",
    "\n",
    "\n",
    "This notebook describes **clustering** and **outlier detection** methods in the unsupervised domain learning.\n",
    "\n",
    "The following contents are covered: \n",
    "\n",
    "- $K$-means \n",
    "- GMM\n",
    "- Model-based outlier detection (based on Mahalanobis distance)\n",
    "- Local Outlier Factor\n",
    "\n",
    "It is highly recommended that this notebook is read and run after a first reading of the theory and in parallel with the slides available in AV. \n",
    "Note also that it is not required to develop any code. All examples are totally implemented, and therefore these notebooks have to be regarded as demonstrative material. The goal is understanding the operation of the algorithms. The notebook contains several questions that have be to submitted through AV. \n",
    "\n",
    "As with Notebooks 1 and 2, the codes used for generating and plotting some of the data sets have been adapted from <a href=https://github.com/ageron/handson-ml2>Geron (Github site)</a>. Please, consult the textbook for reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:35:20.585781Z",
     "start_time": "2020-02-28T18:35:20.580980Z"
    }
   },
   "source": [
    "## K-means\n",
    "\n",
    "**Clustering** algorithms aim at identifying groups (clusters) of *similar* data points.  $K$-means is a clustering algorithm whose intuitive idea is grouping together *close* data points while separating *far* ones.\n",
    "\n",
    "Let $\\pmb{\\mu}_k$ be a $D$-dimensional point representing the $k^\\text{th}$ cluster center (**centroid**), for $k$=$1,2,\\ldots,K$, and let $\\pmb{r}_{nk}$ be binary variables with value 1 if data point $\\pmb{x}_n$ is assigned to cluster $k$, or 0 otherwise.\n",
    "\n",
    "To characterize how good a cluster assignment is, the **cost** $J$ is defined:\n",
    "\n",
    "\\begin{equation}\n",
    "J = \\sum_{n=1}^N \\sum_{k=1}^K \\pmb{r}_{nk} \\|\\pmb{x}_n - \\pmb{\\mu}_k\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "The goal of $K$-means is to find the values $\\{\\pmb{r}_{nk}\\}$ and $\\{\\pmb{\\mu}_k\\}$ minimizing cost $J$.\n",
    "\n",
    "For that, the next iterative algorithm is used:\n",
    "\n",
    "1. Choose random centroids $\\{\\pmb{\\mu}_k\\}$\n",
    "\n",
    "2. For each point, select the closer cluster assuming fixed centroids $\\{\\pmb{\\mu}_k\\}$, updating $\\{\\pmb{r}_{nk}\\}$: \n",
    "\n",
    "\\begin{equation}\n",
    "\\pmb{r}_{nk} = \\begin{cases}\n",
    "1&\\text{if $k$=$\\underset{j}{\\operatorname{argmax}} \\|\\pmb{x}_n - \\pmb{\\mu}_j\\|^2$}\\\\\n",
    "0&\\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "3. Update the centroids $\\{\\pmb{\\mu}_k\\}$ assuming the $\\{\\pmb{r}_{nk}\\}$ are fixed. That is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\pmb{\\mu}_k = \\frac{\\sum_{n=1}^N \\pmb{r}_{nk} \\pmb{x}_n}{\\sum_{n=1}^N \\pmb{r}_{nk}}\n",
    "\\end{equation}\n",
    "\n",
    "4. Go to 2) if solution didn't converge\n",
    "\n",
    "Let's implement this procedure and see its operation with a toy data set. First, some common functions for data visualization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:24:33.033274Z",
     "start_time": "2020-03-04T14:24:32.604732Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "def plot_data(X, R, colors=['r.', 'g.', 'b.', 'k.', 'y.','c.','m.'], markersize=2):\n",
    "    Raux = R.copy()\n",
    "    if R.ndim>1:\n",
    "        K = R.shape[1]\n",
    "        for n in range(R.shape[0]):\n",
    "            i = np.argmax(R[n])\n",
    "            Raux[n] = np.zeros([1,K])\n",
    "            Raux[n,i] = 1\n",
    "    else:\n",
    "        minR = np.amin(R)\n",
    "        K = np.amax(R)-minR+1\n",
    "        Raux = np.zeros([X.shape[0],K])\n",
    "        for n in range(R.shape[0]):\n",
    "            i = R[n] - minR \n",
    "            Raux[n] = np.zeros([1,K])\n",
    "            Raux[n,i] = 1 \n",
    "            \n",
    "    for k in range(Raux.shape[1]):\n",
    "        pattern = np.zeros([K,])\n",
    "        pattern[k] = 1\n",
    "        matches = (Raux==pattern).all(axis=1).nonzero()\n",
    "        plt.plot(X[matches, 0], X[matches, 1], colors[k], markersize=markersize)\n",
    "        \n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=30, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=50, linewidths=50,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True, \n",
    "                             method='sklearn', clusterer=None, \n",
    "                             centroids=None, Sigma=None, pi=None, methodpredict=None):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    if method=='sklearn':\n",
    "        Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        if pi is None:\n",
    "            Z = methodpredict(np.c_[xx.ravel(), yy.ravel()],centroids)\n",
    "        else:\n",
    "            Z = methodpredict(np.c_[xx.ravel(), yy.ravel()],centroids,Sigma,pi)\n",
    "        \n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    \n",
    "    if method=='sklearn':\n",
    "        plot_data(X, clusterer.predict(X))\n",
    "        if show_centroids:\n",
    "            try: \n",
    "                plot_centroids(clusterer.cluster_centers_)\n",
    "            except:\n",
    "                plot_centroids(clusterer.means_)\n",
    "\n",
    "        if show_xlabels:\n",
    "            plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "        else:\n",
    "            plt.tick_params(labelbottom=False)\n",
    "        if show_ylabels:\n",
    "            plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "        else:\n",
    "            plt.tick_params(labelleft=False)\n",
    "    else:\n",
    "        plot_data(X,selectcluster(X,centroids))\n",
    "\n",
    "\n",
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:24:33.047415Z",
     "start_time": "2020-03-04T14:24:33.035286Z"
    }
   },
   "outputs": [],
   "source": [
    "def selectcluster(X,mu):\n",
    "    R = np.zeros([X.shape[0],mu.shape[0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        V = X[i,:]-mu\n",
    "        dist = np.linalg.norm(V, axis=1)\n",
    "        R[i,np.argmin(dist)] = 1\n",
    "    return R\n",
    "\n",
    "def updatecentroids(X,R):\n",
    "    K = R.shape[1]\n",
    "    mu = np.zeros([K, X.shape[1]])\n",
    "    for k in range(K):\n",
    "        pattern = np.zeros([K,])\n",
    "        pattern[k] = 1\n",
    "        matches = (R==pattern).all(axis=1).nonzero()\n",
    "        mu[k,:] = np.mean(X[matches,:], axis=1)\n",
    "    return mu\n",
    " \n",
    "def kmeans(X,K,plot=False):\n",
    "    mu = np.random.permutation(X)[:K]\n",
    "    R = np.zeros([X.shape[0],mu.shape[0]])\n",
    "    \n",
    "    while 1:\n",
    "        R_old = R.copy()\n",
    "        R = selectcluster(X,mu)\n",
    "        mu_old = mu.copy()\n",
    "        mu = updatecentroids(X,R)\n",
    "        if plot:\n",
    "            clear_output(wait=True)\n",
    "            plot_data(X,R)\n",
    "            plot_centroids(mu, circle_color='r', cross_color='y')\n",
    "            plt.show()\n",
    "        if (R_old==R).all() and (mu_old==mu).all(): # Result has converged\n",
    "            break\n",
    "    \n",
    "    return mu,R\n",
    "\n",
    "def predict(X,mu): \n",
    "    R = selectcluster(X,mu)\n",
    "    label = np.argmax(R,axis=1)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run this code with an example data set. Note that the figure shows the clustering process evolution. **Since the centroids are chosen at random at the beginning the results changes from execution to execution**. After the iterative the boundary surfaces separating each cluster (by distance to the corresponding centroid) are show for 2 runs of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:25:19.385270Z",
     "start_time": "2020-03-04T14:24:33.049140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets create some clustered data \n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "\n",
    "X, t = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "\n",
    "# First we can see an interactive K-means running\n",
    "kmeans(X,5,plot=True)\n",
    "\n",
    "# Next, show solutions to M runs \n",
    "M = 2 # Set the number of runs\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "for m in range(M):\n",
    "    mu, R = kmeans(X,5,plot=False)\n",
    "    plt.subplot(211+m) # Set the \"2\" to the M used to address the right plot\n",
    "    plot_decision_boundaries(X, method='mymethod', resolution=500, centroids=mu, methodpredict=predict)\n",
    "    plot_data(X,R)\n",
    "    plot_centroids(mu, circle_color='r', cross_color='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T19:21:21.456882Z",
     "start_time": "2020-03-03T19:21:21.447700Z"
    }
   },
   "source": [
    "<b style=\"color:red\"> Previous cell may take a long time (~2 min) to execute and represent the boundaries. Please, allow enough time for running. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 1\n",
    "> **As you seen in the figures above, the result of a single K-means running is changing from execution to execution. Propose some way to select among all the runs the best solution.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture model\n",
    "\n",
    "A GMM is a superposition of $K$-dimensional Gaussian variables, and has distribution:\n",
    "\\begin{equation}\n",
    "p(\\pmb{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\pmb{x}|\\pmb{\\mu}_k, \\pmb{\\Sigma}_k)\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\{\\pi_k\\}$ are non-negative and satisfy $\\sum_{k=1}^K \\pi_k$ = 1, and $\\mathcal{N}(\\pmb{x}|\\pmb{\\mu},\\pmb{\\Sigma})$ denotes a multi-variate Gaussian random variable.\n",
    "\n",
    "The GMM model can also be understood by assuming a latent (hidden) variable\n",
    "$z$, whose value $k \\in \\{1,\\ldots,K\\}$ selects the $k^\\text{th}$ Gaussian\n",
    "variable $\\mathcal{N}(\\pmb{\\mu}_k, \\pmb{\\Sigma}_k)$ from which the variable $\\pmb{x}$\n",
    "is subsequently drawn.\n",
    "\n",
    "\n",
    "1. Set initial  parameters $\\{\\pi_k, \\pmb{\\mu}_k,\\pmb{\\Sigma}_k\\}$.\n",
    "2. **Expectation**. With the current parameters, evaluate the *responsibilities* $\\{\\gamma_{nk}\\}$: the\n",
    "  probability that the $n^\\text{th}$ data point has been generated by cluster $k$:\n",
    "  \\begin{equation}\n",
    "  \\gamma_{nk} = \\frac{\\pi_k\\mathcal{N}(\\pmb{x}_n|\\pmb{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^K   \\pi_j\\mathcal{N}(\\pmb{x}_n|\\pmb{\\mu}_j,\\Sigma_j)}\n",
    "  \\end{equation}\n",
    "\n",
    "3. **Maximization**. Set $N_k$ = $\\sum_{n=1}^N \\gamma_{nk}$ and re-estimate the parameters:\n",
    "  \\begin{equation}\n",
    "  \\begin{split}\n",
    "  \\pmb{\\mu}_k^\\text{new} & =  \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk}\\pmb{x}_n \\\\\n",
    "  \\pmb{\\Sigma}_k^\\text{new} & =  \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk} (\\pmb{x}_n -   \\pmb{\\mu}_k^\\text{new}) (\\pmb{x}_n - \\pmb{\\mu}_k^\\text{new})^T \\\\\n",
    "  \\pi_k^\\text{new} & =  \\frac{N_k}{N}\n",
    "  \\end{split}\n",
    "  \\end{equation}\n",
    "\n",
    "4. Compute the log-likelihood of the data set:\n",
    "\\begin{equation}\n",
    "\\sum_{n=1}^N ln ( \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\pmb{x}_n|\\pmb{\\mu}_k, \\pmb{\\Sigma}_k))\n",
    "\\end{equation}\n",
    "\n",
    "   Go to 2) if solution didn't converge\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:30:35.562408Z",
     "start_time": "2020-03-04T14:25:19.387724Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def expectation(X,mu,Sigma,pi):\n",
    "    pi = pi.ravel()\n",
    "    gamma = np.zeros([X.shape[0], mu.shape[0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        aux = np.zeros([mu.shape[0],])\n",
    "        for j in range(mu.shape[0]):\n",
    "            try: \n",
    "                aux[j] = pi[j]*multivariate_normal.pdf(X[i], mu[j], Sigma[j])\n",
    "            except np.linalg.LinAlgError as err:\n",
    "                aux[j] = pi[j]/K\n",
    "                \n",
    "        gamma[i] = aux/np.sum(aux)\n",
    "    return gamma \n",
    "\n",
    "def maximization(X,gamma):\n",
    "    K = gamma.shape[1]\n",
    "    N = np.sum(gamma,axis=0)\n",
    "    mu = np.zeros([gamma.shape[1], X.shape[1]])\n",
    "    Sigma = np.zeros([gamma.shape[1], X.shape[1], X.shape[1]])\n",
    "    pi = np.zeros([X.shape[1],])\n",
    "    for k in range(K):\n",
    "        for j in range(X.shape[0]):\n",
    "            mu[k] += gamma[j,k]*X[j]\n",
    "        mu[k] = mu[k]/N[k]\n",
    "        \n",
    "        for j in range(X.shape[0]):\n",
    "            aux = (X[j]-mu[k]).reshape([2,1])\n",
    "            Sigma[k] += gamma[j,k]*aux.dot(aux.T)\n",
    "        Sigma[k] = Sigma[k]/N[k]\n",
    "        if np.sum(Sigma[k])>((np.max(X)-np.min(X))/3)**2: # Reset Sigma\n",
    "            Sigma[k] = Sigma[k]/np.max(Sigma[k]) #np.eye(X.shape[1])\n",
    "        \n",
    "    pi = N/sum(N)\n",
    "    return mu,Sigma,pi\n",
    " \n",
    "def gmm(X,K,plot=False):\n",
    "    # First K-means is run to select an initial solution\n",
    "    mu, R = kmeans(X,K,plot=False)\n",
    "    Sigma = np.zeros([K, X.shape[1], X.shape[1]])\n",
    "    for k in range(K):\n",
    "        Sigma[k] = np.eye(X.shape[1])\n",
    "    pi = (np.ones(K)/K).reshape(-1,1)\n",
    "    gamma = R\n",
    "    J = float('Inf')\n",
    "    \n",
    "    while 1:\n",
    "        gamma_old = gamma.copy()\n",
    "        gamma = expectation(X,mu,Sigma,pi)\n",
    "        mu_old = mu.copy()\n",
    "        Sigma_old = Sigma.copy()\n",
    "        pi_old = pi.copy()\n",
    "        mu, Sigma, pi = maximization(X,gamma)\n",
    "        if plot:\n",
    "            clear_output(wait=True)\n",
    "            plot_data(X,gamma)\n",
    "            plot_centroids(mu, circle_color='r', cross_color='y')\n",
    "            plt.show()\n",
    "            \n",
    "        J_old = J\n",
    "        aux = np.zeros([X.shape[0],])\n",
    "        for i in range(X.shape[0]):\n",
    "            aux[i] = 0\n",
    "            for j in range(mu.shape[0]):\n",
    "                try: \n",
    "                    aux[i] += pi[j]*multivariate_normal.pdf(X[i], mu[j], Sigma[j])\n",
    "                except np.linalg.LinAlgError as err:\n",
    "                    aux[i] += pi[j]/K\n",
    "            aux[i] = np.log(aux[i])\n",
    "        J = np.sum(aux)\n",
    "            \n",
    "        if (J-J_old)**2 < 0.01: # Convergence\n",
    "            break\n",
    "    \n",
    "    return mu,Sigma,pi,gamma\n",
    "\n",
    "def predictgmm(X,mu,Sigma,pi): \n",
    "    gamma = expectation(X,mu,Sigma,pi)\n",
    "    label = np.argmax(gamma,axis=1)\n",
    "    return label\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "mu, Sigma, pi, gamma = gmm(X,5,plot=True)\n",
    "plot_decision_boundaries(X, method='mymethod', resolution=500, centroids=mu, Sigma=Sigma, pi=pi, methodpredict=predictgmm)\n",
    "plot_data(X,gamma)\n",
    "plot_centroids(mu, circle_color='r', cross_color='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:red\"> Previous cell may take a long time (~5 min) to execute and represent the boundaries. Please, allow enough time for running. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with sklearn\n",
    "\n",
    "Next, we will examine how to create clusters directly with the sklearn library. For that, the classes **KMeans** and **GaussianMixture** have to be used. Their syntax is straightforward and very similar to that of the supervised methods. The running times are also **much faster** than the python-versions shown above, since sklearn is internally using optimized libraries written in C and C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:30:43.363199Z",
     "start_time": "2020-03-04T14:30:35.564249Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(X)\n",
    "plt.subplot(211)\n",
    "plot_decision_boundaries(X, method='sklearn', clusterer=kmeans)\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=5, random_state=0).fit(X)\n",
    "plt.subplot(212)\n",
    "plot_decision_boundaries(X, method='sklearn', clusterer=gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (distance-based) outlier detection\n",
    "\n",
    "\n",
    "The core idea of these methods is determining how much unexpected an observation $\\pmb{x}$ is, given a previously observed data set $\\{\\pmb{x}_n\\}$ by computing the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance): \n",
    "\n",
    "\\begin{equation}\n",
    "d_\\mathcal{M} =  [(\\pmb{x}-\\widehat{\\pmb{\\mu}})^T \\widehat{\\pmb{\\Sigma}}^{-1} (\\pmb{x}-\\widehat{\\pmb{\\mu}})]^{1/2}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\widehat{\\pmb{\\mu}} & = \\frac{1}{N}\\sum_{n=1}^N\\pmb{x}_n \\\\\n",
    "\\widehat{\\pmb{\\Sigma}} & = \\frac{1}{N-1} \\sum_{n=1}^N (\\pmb{x}_n -\\widehat{\\pmb{\\mu}})(\\pmb{x}_n -\\widehat{\\pmb{\\mu}})^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This distance is commonly used as a measure of the novelty of a point compared with a clustered data set or distribution. Next examples show how this algorithm detects outliers in different setups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:30:43.593197Z",
     "start_time": "2020-03-04T14:30:43.365312Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blob_centers = np.array([[0.0,  0.0]])\n",
    "blob_std = np.array([[0.2]])\n",
    "\n",
    "X, t = make_blobs(n_samples=200, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "n_outliers = X_outliers.shape[0]\n",
    "X = np.r_[X, X_outliers]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_clusters(X,'b')\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "gmm = GaussianMixture(n_components=1, random_state=0).fit(X) # 1 Gaussiana = Mahalanobis\n",
    "#y = multivariate_normal.pdf(X, gmm.means_[0], gmm.covars_[0])\n",
    "y = gmm.score_samples(X)\n",
    "condition = y<np.percentile(y,10)\n",
    "outliers = X[condition] # 10% of the most anomalous points\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], s=10*y[condition]/np.percentile(y,10), edgecolors='y', facecolors='y', label='Outlier scores')\n",
    "plot_clusters(X_outliers,'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot marks as outliers the 10% of the most unlikely points. The **bigger the circled area the higher the chances that the point is a real outlier**. Points in the circled areas with the inner point in black are those not generated from the Gaussian distribution (real outliers). The case for a multi-cluster data is shown next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:30:43.817472Z",
     "start_time": "2020-03-04T14:30:43.595095Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blob_centers = np.array([[0.0,  0.0], [1.0, 1.0], [2.0, 0.0]])\n",
    "blob_std = np.array([[0.2], [0.3], [0.2]])\n",
    "\n",
    "X, t = make_blobs(n_samples=200, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "\n",
    "X_outliers = np.random.uniform(low=-1, high=3, size=(20, 2))\n",
    "n_outliers = X_outliers.shape[0]\n",
    "X = np.r_[X, X_outliers]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_clusters(X,'b')\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "gmm = GaussianMixture(n_components=1, random_state=0).fit(X) # 1 Gaussiana = Mahalanobis\n",
    "#y = multivariate_normal.pdf(X, gmm.means_[0], gmm.covars_[0])\n",
    "y = gmm.score_samples(X)\n",
    "condition = y<np.percentile(y,10)\n",
    "outliers = X[condition] # 10% of the most anomalous points\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], s=10*y[condition]/np.percentile(y,10), edgecolors='y', facecolors='y', label='Outlier scores')\n",
    "plot_clusters(X_outliers,'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Outlier Factor\n",
    "\n",
    "Local Outlier Factor (LOF) is a **local density** method, performed over the $K$ nearest neighbors, whose distance is used to estimate the point density. When that density is much lower than that of the neighbors, a point is considered an outlier.\n",
    "\n",
    "Let $k$-distance$(\\pmb{x})$ be the distance of a point $\\pmb{x}$ to its $k^\\text{th}$ nearest neighbor, and let $N_K(\\pmb{x})$ be the set of neighbors at distance less or equal than $K$-distance$(\\pmb{x})$. Besides, let the **reachability** between two points $\\pmb{x}, \\pmb{x}'$ be $r_K$=$\\max\\{K\\text{-distance}(\\pmb{x}), d(\\pmb{x},\\pmb{x}')\\}$.\n",
    "\n",
    "Then, the **local reachability density** of $\\pmb{x}$ is defined as:\n",
    "\\begin{equation}\n",
    "\\rho_K(\\pmb{x}) = \\left(\\frac{\\sum_{\\pmb{x}'\\in N_K(\\pmb{x})} r_K(\\pmb{x},\\pmb{x}'\n",
    ")}{|N_K(\\pmb{x})|}\\right)^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "and represents the inverse of the distance at which $\\pmb{x}$ can be reached **from** its neighbors.\n",
    "\n",
    "Finally, it is possible to compare the local density with that of the neighbors by computing:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{LOF}_K(\\pmb{x}) = \\frac{\\sum\\limits_{\\pmb{x}' \\in N_K(\\pmb{x})} \\rho_K(\\pmb{\n",
    "x}')}{|N_K(\\pmb{x})| \\rho_K(\\pmb{x})}\n",
    "\\end{equation}\n",
    "\n",
    "**A ratio greater than a selected LOF$_{\\text{critical}}$ identifies an outlier**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:30:44.039143Z",
     "start_time": "2020-03-04T14:30:43.819968Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blob_centers = np.array([[0.0,  0.0]])\n",
    "blob_std = np.array([[0.2]])\n",
    "\n",
    "X, t = make_blobs(n_samples=200, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "X = np.r_[X, X_outliers]\n",
    "\n",
    "# fit the model for outlier detection (default)\n",
    "clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "y_pred = clf.fit_predict(X)\n",
    "X_scores = clf.negative_outlier_factor_\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Local Outlier Factor (LOF)\")\n",
    "plt.scatter(X[:, 0], X[:, 1], color='b', s=3., label='Data points')\n",
    "radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
    "cond = radius>np.percentile(radius,90) # 10% of the highest radius\n",
    "plt.scatter(X[cond, 0], X[cond, 1], s=radius[cond]*1000, edgecolors='y', facecolors='y', label='Outlier scores')\n",
    "plot_clusters(X_outliers,'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, **in LOF we can use the local density to provide a score with the likelihood that a points is a real outlier**. The sa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:30:44.259501Z",
     "start_time": "2020-03-04T14:30:44.041365Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blob_centers = np.array([[0.0,  0.0], [1.0, 1.0], [2.0, 0.0]])\n",
    "blob_std = np.array([[0.2], [0.3], [0.2]])\n",
    "\n",
    "X, t = make_blobs(n_samples=200, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "\n",
    "X_outliers = np.random.uniform(low=-1, high=3, size=(20, 2))\n",
    "X = np.r_[X, X_outliers]\n",
    "\n",
    "# fit the model for outlier detection (default)\n",
    "clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "y_pred = clf.fit_predict(X)\n",
    "X_scores = clf.negative_outlier_factor_\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Local Outlier Factor (LOF)\")\n",
    "plt.scatter(X[:, 0], X[:, 1], color='b', s=3., label='Data points')\n",
    "radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
    "cond = radius>np.percentile(radius,90) # 10% of the highest radius\n",
    "plt.scatter(X[cond, 0], X[cond, 1], s=radius[cond]*1000, edgecolors='y', facecolors='y', label='Outlier scores')\n",
    "plot_clusters(X_outliers,'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 2\n",
    "> **In a real problem, it is often useful to combine clustering and outlier filtering in unlabeled data. In which order you have to perform that operations and why?**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full example on the MNIST data set\n",
    "\n",
    "In this last section we show how to use unsupervised learning on a more complex data set: the MNIST. For that, the following procedure is used: \n",
    "\n",
    "1. To reduce run time a random subset of the instances is selected (20000 instances).\n",
    "2. A GMM clustering takes place on this data and the images clustered together are shown.\n",
    "3. An outlier detection of the 0.1% more likely outliers is performed using LOF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:31:07.945285Z",
     "start_time": "2020-03-04T14:30:44.261608Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "# print(mnist.DESCR) # Uncomment to show description\n",
    "X, t = mnist[\"data\"], mnist[\"target\"] # N = 70000, D = 784 \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, t = shuffle(X, t, random_state=0)\n",
    "X, t = X[:10000], t[:10000]\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.plasma, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:32:28.453878Z",
     "start_time": "2020-03-04T14:31:07.947330Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=10, random_state=0).fit(X)\n",
    "y = gmm.predict(X) # Assigns to each image a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:red\"> Previous cell may take a long time (~2 min) to execute and represent the boundaries. Please, allow enough time for running. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:32:28.549516Z",
     "start_time": "2020-03-04T14:32:28.455496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's show the first 100 images clustered together for one class \n",
    "# to see how good has been the result\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plot_digits(X[y==9][:100], images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:32:28.645736Z",
     "start_time": "2020-03-04T14:32:28.551007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's show the first 100 images clustered together for another class \n",
    "# to see how good has been the result\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plot_digits(X[y==1][:100], images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not bad!! This process can be used for data pre-labeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:34:55.284411Z",
     "start_time": "2020-03-04T14:32:28.647826Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "y_pred = clf.fit_predict(X)\n",
    "X_scores = clf.negative_outlier_factor_\n",
    "radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
    "cond = radius>np.percentile(radius,99.9) # 0.1% of the highest radius -> 8 images\n",
    "\n",
    "outliers = X[cond]\n",
    "plot_digits(outliers, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:red\"> Previous cell may take a long time (~2 min) to execute and represent the boundaries. Please, allow enough time for running. </b>\n",
    "\n",
    "**See how the result reveals some oddly drawn characters!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 3\n",
    "> **Provide some example of unsupervised learning usage in your research topic, and explain which of the previous algorithms you would use.**\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1163px",
    "left": "1185.6px",
    "top": "111.133px",
    "width": "344.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
