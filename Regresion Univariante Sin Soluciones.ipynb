{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  <head>\n",
    "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
    "    <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n",
    "    <meta name=\"generator\" content=\"pandoc\" />\n",
    "    <title></title>\n",
    "    <style type=\"text/css\">code{white-space: pre;}</style>\n",
    "  </head>\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "<h1> AMI </h1> \n",
    "<h1> Aprendizaje Supervisado: Regresores </h1>\n",
    "\n",
    "<br>\n",
    "    \n",
    "En esta práctica implementaremos en Python sistemas regresores. Como se ha indicado este mecanismo es utilizado en el contexto del aprendizaje máquina para predecir el valor de variables de salida continuas y es de tipo supervisado ya que debemos contar con el valor de salida asociado a cada conjunto de entradas. \n",
    "\n",
    "En la próxima práctica estudiaremos el caso de variables de salida discreta (sistemas clasificadores), y para ello\n",
    "nos valdremos de los conceptos estudiados durante esta primera práctica. \n",
    "\n",
    "Para la implementación de mecanismos de aprendizaje máquina es habitual enfrentarnos a algún problema de optimización. Por ejemplo, elegir los parámetros de la hipótesis h<sub>θ</sub>(x) de tal modo que el coste sea el menos por posible. De los diversos mecanismos posibles de optimización, nosotros implementaremos y usaremos el método de las ecuaciones normales y el descenso del gradiente en esta práctica.\n",
    "\n",
    "Al finalizar veremos asimismo como mediante las librerías de Python podemos resolver rápidamente problemas de este tipo. Internamente hará uso de soluciones similares a las que nosotros hemos desarrollado paso a paso. \n",
    "\n",
    "<h2> Regresores lineales univariantes </h2>\n",
    "\n",
    "Comenzaremos con el caso más sencillo, un regresor con una sóla variable independiente (<em>feature</em>) en terminología de aprendizaje máquina y una sóla salida (<em>target</em>). Primero cargaremos algunas librerías y generaremos un dataset de prueba, y lo mostramos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Para generar figuras\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Establecemos la semilla para generar siempre los mismos resultados\n",
    "np.random.seed(1)\n",
    "\n",
    "# Generamos dataset para pruebas\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Mostramos el dataset\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Aproximación lineal sin término independiente </h3>\n",
    "\n",
    "La hipótesis lineal pura es de la forma:  h<sub>θ</sub>(x) = θx, siendo θ un escalar. Vamos a buscar el mejor valor para θ a la vista de los datos. Para ello queremos minimizar la función de coste J(θ) (el MSE) entre los valores reales y la aproximación. \n",
    "\n",
    "Es decir:\n",
    "\n",
    "<p><font size=\"+1\"><span class=\"math inline\">$J(\\theta)$=MSE=$\\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - h_{\\theta}(x^{(i)})^2$</span></font></p>\n",
    "\n",
    "Sustituyendo la función de hipótesis: \n",
    "\n",
    "<p><font size=\"+1\"><span class=\"math inline\">$J(\\theta)$=$\\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - \\theta x^{(i)})^2$</span></font></p>\n",
    "\n",
    "Esta expresión la podemos \"compactar\" escribiendola con matrices y vectores:\n",
    "\n",
    "<p><font size=\"+1\"><span class=\"math inline\">$J(\\theta)$=$\\frac{1}{m}\\;(Y-X\\theta)'(Y-X\\theta)$</span></font></p>\n",
    "\n",
    "<h4> Cuestión: Proporcione la fórmula y complete el código Python para obtener el θ óptimo en una regresión lineal sin término independiente</h4>\n",
    "\n",
    "Es decir, debe obtener: <font size=\"+1\"><span class=\"math inline\">$\\min\\limits_{\\theta} J(\\theta)$</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución:\n",
    "    \n",
    "En este caso deberíamos derivar la expresión anterior e igualarla a cero. Es decir: \n",
    "    \n",
    "\n",
    "<p><font size=\"+1\"><span class=\"math inline\">$\\frac{d J(\\theta)}{d \\theta}=-\\frac{1}{m}(X'(Y-X\\theta) + ((Y-X\\theta)'X)') = -\\frac{1}{m}(X'Y - X'X\\theta + X'Y - X'X\\theta) = -\\frac{2}{m}(X'Y - X'X\\theta) = 0$</span></font></p>\n",
    "\n",
    "Despejando θ obtenemos:\n",
    "\n",
    "<p><font size=\"+1\"><span class=\"math inline\">$\\theta=(X'X)^{-1}X'Y$</span></font></p>\n",
    "\n",
    "Nota 1: Aunque X no es invertible (X'X) si puede serlo al ser cuadrada. Esta ecuación de resolución se denomina <b>ecuación normal</b>\n",
    "\n",
    "Nota 2: Al trabajar con matrices observe que <font size=\"+1\"><span class=\"math inline\">$d(AB) = (dA) B + (A (dB))'$</span></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best = # DEBE COMPLETAR \n",
    "theta_best\n",
    "X_new = np.array([[0], [2]])\n",
    "y_predict = # DEBE COMPLETAR\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Aproximación afín (lineal con término independiente) </h3>\n",
    "\n",
    "<h4> Cuestión: Repita el mismo proceso que antes, pero suponiendo que la hipótesis es una función lineal con un termino independiente </h4>\n",
    "\n",
    "<br>\n",
    "<p><font size=\"+1\"><span class=\"math inline\">$h_{\\theta} (x) = \\theta_0 + \\theta_1 x$</span></font></p>\n",
    "\n",
    "Proporcione la fórmula y el código Python para obtener el vector <font size=\"+0.5\"><span class=\"math inline\">$\\theta =  \\begin{pmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1\n",
    "\\end{pmatrix}$</span></font> óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución: \n",
    "\n",
    "DEBE COMPLETAR ESTE APARTADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]  # Añadimos bias (x0 = 1) a cada instancia\n",
    "\n",
    "theta_best = # DEBE COMPLETAR \n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # Añadimos bias (x0 = 1) a cada instancia\n",
    "y_predict = # DEBE COMPLETAR\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()\n",
    "\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Regresión usando el método del descenso del gradiente</h3>\n",
    "\n",
    "En general no va a ser posible obtener directamente los parámetros de θ óptimos, o hacerlo por el método de las ecuaciones normales puede ser muy lento. Vamos a ver por qué en el siguiente ejercicio: \n",
    "\n",
    "<h4>Cuestión: Obtenga el tiempo de ejecución necesario para calcular el θ óptimo para m=100, m=1000, m=10000 y m=100000 donde m es el número de instancias</h4>\n",
    "\n",
    "Nota 3: En ipython puede obtener el tiempo de ejecución de una instrucción predeciendola de la función \"mágica\" %timeit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [100,1000,10000,100000]:\n",
    "    X_test = 2 * np.random.rand(m, 1)\n",
    "    y_test = 4 + 3 * X_test + np.random.randn(m, 1)\n",
    "    X_test_b = np.c_[np.ones((m, 1)), X_test]  # Añadimos bias (x0 = 1) a cada instancia\n",
    "    print(m)\n",
    "    # DEBE COMPLETAR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método del descenso del gradiente es una alternativa al cálculo explicito del óptimo. Como hemos visto en teoría se basa en corregir sucesivamente el punto de trabajo, <font size=\"+1\"><span class=\"math inline\">$\\theta_i$</span></font>, moviendolo hacia la dirección contraría al máximo crecimiento de la función de pérdida. Es decir, hacía <font size=\"+1\"><span class=\"math inline\">$-\\nabla J(\\theta_i)$</span></font>, de tal modo que se actualice el punto de trabajo a: \n",
    "\n",
    "<p><font size=\"+0.5\"><span class=\"math inline\">$\\theta_{i+1} = \\theta_i - \\alpha \\nabla J(\\theta_i)$</span></font></p>\n",
    "\n",
    "Donde <font size=\"+1\"><span class=\"math inline\">$\\alpha$</span></font> es el factor de aprendizaje (learning rate) y controla la velocidad de convergencia del algoritmo. Cuando mayor menos pasos habrá que dar hasta la convergencia, si bien, si es demasiado elevado puede hacer que el método no converja. \n",
    "\n",
    "<h4> Cuestión: Complete el siguiente código con el cálculo del gradiente </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "n_iterations = 1000\n",
    "theta = np.random.randn(2,1) # Punto inicial se escoge al azar\n",
    "\n",
    "def gradient(X_b,y,theta):\n",
    "#DEBE COMPLETAR\n",
    "             \n",
    "for iteration in range(n_iterations):\n",
    "    gradients = gradient(X_b,y,theta)\n",
    "    theta = theta - alpha * gradients\n",
    "\n",
    "y_predict_grad = # DEBE COMPLETAR\n",
    "print(y_predict_grad, y_predict) # Prediccion para los puntos x=0.0 y x=2.0,  comparamos con el anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a mostrar la influencia del learning rate, mostrando la solución obtenida iterativamente para diferentes valores de α:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_bgd = []\n",
    "\n",
    "def plot_gradient_descent(theta, alpha, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = # DEBE COMPLETAR\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        theta = theta - alpha * gradient(X,y,theta)\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\alpha = {}$\".format(alpha), fontsize=16)\n",
    "    \n",
    "np.random.seed(11)    \n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "    \n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, alpha=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, alpha=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Cuestión: Cálcule el tiempo de convergencia empleando los mismos números de instancias que en el caso anterior </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "n_iterations = 1000\n",
    "\n",
    "import timeit\n",
    "\n",
    "for m in [100,1000,10000,100000]:\n",
    "    X_test = 2 * np.random.rand(m, 1)\n",
    "    y_test = 4 + 3 * X_test + np.random.randn(m, 1)\n",
    "    X_test_b = np.c_[np.ones((m, 1)), X_test]  # Añadimos bias (x0 = 1) a cada instancia\n",
    "    print(m)\n",
    "    start_time = timeit.default_timer()\n",
    "    # DEBE COMPLETAR\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Regresión usando la librería scikit-learn </h3>\n",
    "\n",
    "A continuación mostraremos el uso de las librerías scikit-learn de Python para poder abordar directamente problemas de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "y_predict_sklearn = lin_reg.predict(X_new) # Valor en x=0.0 y x=2.0, comparamos con el anterior\n",
    "print(y_predict, y_predict_grad, y_predict_sklearn) # Deberían coincidir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h2> Regresores polínomicos univariantes </h2>\n",
    "\n",
    "Para finalizar la sesión de hoy estuadiaremos el caso de un regresor con una sóla <em>feature</em>, donde la función de hipótesis va a ser de tipo polínomico de grado P, es decir:\n",
    "\n",
    "<br>\n",
    "<p><font size=\"+1\"><span class=\"math inline\">$h_{\\theta} (x) = \\sum_{p=1}^P \\theta_p x^p$</span></font></p>\n",
    "\n",
    "Para estudiar este caso, crearemos un nuevo dataset donde una regresión polínomica sea más adecuada:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "np.random.seed(1)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cuestión: Obtenga la función de coste <span class=\"math inline\">$J(\\theta)$</span> y el gradiente <span class=\"math inline\">$\\nabla J(\\theta)$</span> para este caso. Represente el resultado para P desde 1 hasta 5.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBE COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Regresión polinómica usando la librería scikit-learn </h3>\n",
    "\n",
    "A continuación mostraremos el uso de las librerías scikit-learn de Python para poder abordar directamente la regresión polinómica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False) # Configuramos el modelo\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "in_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(X_poly, y) # Entrenamos el modelo\n",
    "\n",
    "# Mostramos el resultado\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Función hipótesis\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1> Funciones útiles de Python para el desarrollo de la práctica </h1>\n",
    "\n",
    "<ul>\n",
    "<li> Multiplicacion matrices A y B con numpy: A.dot(B)\n",
    "<li> Traspuesta de matriz A con numpy: A.T\n",
    "<li> Inversión de matriz A con numpy: np.linalg.inv(A)\n",
    "<li> Matriz A por vector 𝜃 con numpy: A.dot(theta)\n",
    "<li> Medida de tiempo de una orden con ipython: %timeit ...\n",
    "<li> Medida de tiempo de un bloque con python: <br>\n",
    "    start_time = timeit.default_timer()<br>\n",
    "    ...<br>\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
