{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING FOR RESEARCHERS\n",
    "\n",
    "# Notebook 4. Dimensionality reduction methods\n",
    "\n",
    "\n",
    "\n",
    "This notebook describes **dimensionality reduction** methods in the **unsupervised domain** learning.\n",
    "\n",
    "The following contents are covered: \n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Autoencoders (with ANNs) (*extra material not covered in the Unit 3 slides*)\n",
    "\n",
    "It is highly recommended that this notebook is read and run after a first reading of the theory and in parallel with the slides available in AV. \n",
    "Note also that it is not required to develop any code. All examples are totally implemented, and therefore these notebooks have to be regarded as demonstrative material. The goal is understanding the operation of the algorithms. The notebook contains several questions that have be to submitted through AV. \n",
    "\n",
    "As with previous notebooks, some of the codes used for plotting the data sets have been adapted from <a href=https://github.com/ageron/handson-ml2>Geron (Github site)</a>. Please, consult that textbook for reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:35:20.585781Z",
     "start_time": "2020-02-28T18:35:20.580980Z"
    }
   },
   "source": [
    "## PCA\n",
    "\n",
    "**Dimensionality reduction** algorithms aim at coding *automatically* the data from a high dimensional space to a small dimensional *feature space*. This data processing is necessary because data sets tend to be extremely \\emph{sparse} when working in high dimensionality spaces, and thus reducing dimensions is \\emph{critical} to increase predictions' reliability by avoiding large extrapolations about data.\n",
    "\n",
    "PCA operates given a set of **unlabeled** observations $\\{\\pmb{x}_n\\}$ for $n$ = $1,\\ldots,N$, each of dimensionality $D$. The aim of PCA can be understood in a dual way as either:\n",
    "\n",
    "- Maximizing the variance of the projected data (that is, the information kept by the data) over a feature space of dimensionality $M$<$D$.\n",
    "- Minimizing the projection error in the data reconstruction. \n",
    "\n",
    "Both lead to exactly the same procedure, which involves finding the eigenvectors of the covariance matrix: \n",
    "\n",
    "\\begin{equation}\n",
    "\\pmb{S} = \\frac{1}{N} \\sum_{n=1}^N (\\pmb{x}_n - \\overline{\\pmb{x}})(\\pmb{x}_n - \\overline{\\pmb{x}})^T\n",
    "\\end{equation}\n",
    "\n",
    "That is, solving:\n",
    "\\begin{equation}\n",
    "\\pmb{u}_m^T \\pmb{S} \\pmb{u}_m + \\lambda_m (1 - \\pmb{u}_m^T \\pmb{u}_m).\n",
    "\\end{equation}\n",
    "\n",
    "for $m$=$1,\\ldots,M$. \n",
    "\n",
    "These eigenvectors are chosen to have unit length and they determine the **projection directions** in the input space. Thus, the projection of point $\\pmb{x}$ has length $\\pmb{x}^T\\pmb{u_i}$=$\\pmb{u_i}^T\\pmb{x}$ in direction $\\pmb{u_i}$, and the projected point is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\pmb{x}}_n=\\overline{\\pmb{x}} + \\sum_{i=1}^M [\\pmb{u}_i^T (\\pmb{x}_n-\\overline{\\pmb{x}})] \\pmb{u}_i\n",
    "\\end{equation}\n",
    "\n",
    "This projection has a (hopefully small) error compared to the original point. This error depends on how many dimensions ($M$) are chosen in the feature space. The goal of PCA is thus to determine this basis. For that, either $M$ is fixed with some criteria (e.g., $M$=2 for data visualization) or via an educated guess, or, more commonly, the value of **$M$ is determined such the sum of the $M$ eigenvalues reaches some ratio (say 90%, 95%) of the total sum of the eigenvalues**. This last procedure is justified in the observation that the largest the eigenvalues, the best is the associated eigenvector to project the data (less projection error).\n",
    "\n",
    "Next, we show how to apply PCA to the MNIST data set with the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:33:52.853960Z",
     "start_time": "2020-04-11T12:33:30.263545Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "# print(mnist.DESCR) # Uncomment to show description\n",
    "X, t = mnist[\"data\"], mnist[\"target\"] # N = 70000, D = 784 \n",
    "t = t.astype(int)\n",
    "X_train, X_test, t_train, t_test = X[:60000], X[60000:], t[:60000], t[60000:]\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "#from sklearn.utils import shuffle\n",
    "#X, t = shuffle(X, t, random_state=0)\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.plasma, **options)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "\n",
    "def plot_data(X, R, colors=['r.', 'g.', 'b.', 'k.', 'y.','c.','m.'], markersize=2):\n",
    "    Raux = R.copy()\n",
    "    if R.ndim>1:\n",
    "        K = R.shape[1]\n",
    "        for n in range(R.shape[0]):\n",
    "            i = np.argmax(R[n])\n",
    "            Raux[n] = np.zeros([1,K])\n",
    "            Raux[n,i] = 1\n",
    "    else:\n",
    "        minR = np.amin(R)\n",
    "        K = np.amax(R)-minR+1\n",
    "        Raux = np.zeros([X.shape[0],K])\n",
    "        for n in range(R.shape[0]):\n",
    "            i = R[n] - minR \n",
    "            Raux[n] = np.zeros([1,K])\n",
    "            Raux[n,i] = 1 \n",
    "            \n",
    "    for k in range(Raux.shape[1]):\n",
    "        pattern = np.zeros([K,])\n",
    "        pattern[k] = 1\n",
    "        matches = (Raux==pattern).all(axis=1).nonzero()\n",
    "        plt.plot(X[matches, 0], X[matches, 1], colors[k], markersize=markersize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:33:55.578821Z",
     "start_time": "2020-04-11T12:33:52.855757Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "Phi_train = pca.fit_transform(X_train)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plot_data(Phi_train[:1000], t[:1000], colors=['r+', 'g.', 'k.', 'k+', 'y+','c*','m.','r*','b*','g*','c*'], markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although some *clusters* can be intuited, it is clear that we need more dimensions than 2 to have a separable representation. We can ask PCA to provide, e.g., 95% of the spectral energy (the sum of eigenvalues), show some information, and do a SVM classification in the featured space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:07.028611Z",
     "start_time": "2020-04-11T12:33:55.580709Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_85 = PCA(n_components=0.85) # 85% of the energy\n",
    "Phi_85 = pca_85.fit_transform(X_train)\n",
    "pca_50 = PCA(n_components=0.5) # 50% of the energy\n",
    "Phi_50 = pca_50.fit_transform(X_train)\n",
    "print(f'For 85% of energy -> M:{pca_85.n_components_}, original dimension:{X.shape[1]}')\n",
    "print(f'For 50% of energy -> M:{pca_50.n_components_}, original dimension:{X.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:07.088569Z",
     "start_time": "2020-04-11T12:34:07.030365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show mean\"digit\", the same 50% for 85%\n",
    "meandigit = pca_85.mean_\n",
    "plot_digit(meandigit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:07.236375Z",
     "start_time": "2020-04-11T12:34:07.090033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show eigen\"digits\" (the directions for better projection)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_digits(pca_85.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:07.324254Z",
     "start_time": "2020-04-11T12:34:07.237803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show eigen\"digits\" (the directions for better projection)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_digits(pca_50.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy of firsts eigenvalues is higher, thus, the associated eigendigits resemble more clearly a 'digit'. Note that the eigendigits are the same, the difference is that with 50% of spectral energy more components are required.\n",
    "\n",
    "**To compute the coordinate in the feature space, each of the eigendigtis is multiplied (scalar multiplication) by the digit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:07.443244Z",
     "start_time": "2020-04-11T12:34:07.325833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's project digits and show the results\n",
    "# First: original digits\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_digits(X_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:07.662057Z",
     "start_time": "2020-04-11T12:34:07.445401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's project digits and show the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "X_back = pca_85.inverse_transform(pca_85.transform(X_test))\n",
    "plot_digits(X_back[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:07.859253Z",
     "start_time": "2020-04-11T12:34:07.663736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's project digits and show the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "X_back = pca_50.inverse_transform(pca_50.transform(X_test))\n",
    "plot_digits(X_back[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:17.136148Z",
     "start_time": "2020-04-11T12:34:07.860804Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Solver is lbfgs which uses Ridge regularization by default\n",
    "clf = LogisticRegression(random_state=0, multi_class='multinomial',max_iter=10000).fit(Phi_85, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:17.287160Z",
     "start_time": "2020-04-11T12:34:17.137692Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'The score (accurary) in the training set is {clf.score(Phi_85,t_train)*100:.02f}%')\n",
    "print(f'The score (accurary) in the test set is {clf.score(pca_85.transform(X_test),t_test)*100:.02f}%')\n",
    "\n",
    "plot_digits(X_test[:10], images_per_row=10)\n",
    "print('Labels predicted for the 10 first pictures in the test set')\n",
    "print(clf.predict(pca_85.transform(X_test)[:10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result is similar to the softmax regression, but the classifier took 5 times less time to train.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As aditional example, we show the eigenvectors associated to a 3D toy data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:34:17.563202Z",
     "start_time": "2020-04-11T12:34:17.288660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dado el siguiente conjunto de puntos, \n",
    "# dibujar en rojo los dos primeros vectores principales,\n",
    "# centrados en el punto promedio\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from sklearn.datasets import make_blobs\n",
    "X,_ = make_blobs(n_samples=[500], centers=[[1,1,1]], cluster_std=0.4, random_state=0)\n",
    "X = X.dot([[1.5,-1.3,1],[-1.3,1.4,0.4],[0.1,0.8,0.3]])\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(35, 35)\n",
    "ax.scatter3D(X[:,0], X[:,1], X[:,2], c=X[:,0], cmap='Blues')\n",
    "\n",
    "ax.set_xlim(-2, 3); ax.set_ylim(-2, 3); ax.set_zlim(0, 5);\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) \n",
    "Phi = pca.fit_transform(X)\n",
    "mean = pca.mean_\n",
    "components = pca.components_*1.5\n",
    "ax.quiver(mean[0], mean[1], mean[2], components[0,0], components[0,1], components[0,2], linewidths=1.5, color='red', pivot='tail')\n",
    "ax.quiver(mean[0], mean[1], mean[2], components[1,0], components[1,1], components[1,2], linewidths=1.5, color='red', pivot='tail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we cover an structure called **autoencoder**, which was introduced in Unit 2, along with other unsupervised methods, and that is targeted to dimensionality reduction and other applications like noise-filtering.\n",
    "\n",
    "An autoencoder is a parametric structure (most typically an ANN) which transforms the data inputs $\\pmb{x}_n$ into a **coded form $c(\\pmb{x},\\pmb{w_c})$**, and then back to the input space $\\widetilde{\\pmb{x}}_n$= $d(c(\\pmb{x}_n, \\pmb{w_c}),\\pmb{w_d})$. Since the coding is of (much) lower dimensionality than the input space, some reconstruction noise $ J_n $ = $\\|\\pmb {x} _n- \\widetilde {\\pmb {x}} _ n\\|^2 $ appears.\n",
    "\n",
    "The autoencoder parameters $\\pmb{w_c}$, $\\pmb{w_d}$ are found by minimizing, e.g., the average modulus data reconstruction noise ($J$):\n",
    "\\begin{equation}\n",
    "\\min \\limits _ {\\pmb {w_c}, \\pmb {w_d}} \\frac {1} {2} \\sum_ {n = 1} ^ N J_n\n",
    "\\end{equation}\n",
    "\n",
    "The encoder and decoder parameters are found by **minimizing the joint reconstruction error of a training set**, i.e., the sum of the differences between the original data and the reconstructed one. \n",
    "\n",
    "An easy way to construct autoencoders is using the high-level **Keras** library to define ANNs. It uses some other low-level library to build the layers and the connections, like **tensorflow**, but conveniently hiding all the details to the network designer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:35:45.570533Z",
     "start_time": "2020-04-11T12:34:17.564650Z"
    }
   },
   "outputs": [],
   "source": [
    " # Entrenamos un autoencoder construido en keras\n",
    "import keras\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, \n",
    "                                                  restore_best_weights=True)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"autoencoder.h5\", \n",
    "                                                save_best_only=True)\n",
    "\n",
    "repeat = True # Add another 300 training epochs\n",
    "\n",
    "autoencoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, input_shape=[X_train.shape[1]], activation=\"tanh\"), \n",
    "    keras.layers.Dense(10, input_shape=[50], activation=\"tanh\"),\n",
    "    keras.layers.Dense(50, input_shape=[10], activation=\"tanh\"),\n",
    "    keras.layers.Dense(X_train.shape[1], input_shape=[50], activation=\"tanh\")\n",
    "])\n",
    "    \n",
    "try: \n",
    "    autoencoder = keras.models.load_model(\"autoencoder.h5\") # cargar modelo\n",
    "    print('Modelo cargado')\n",
    "    if repeat: \n",
    "        raise IOError\n",
    "except IOError:\n",
    "    # Code of length 11 as the PCA with 50% of energy\n",
    "    autoencoder.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    history = autoencoder.fit(X_train, X_train, epochs=30, batch_size=100, \n",
    "                              validation_data=(X_test, X_test), \n",
    "                              callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:35:45.866217Z",
     "start_time": "2020-04-11T12:35:45.577099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's project digits and show the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "X_back = autoencoder.predict(X_test[:10])\n",
    "plt.subplot(211)\n",
    "plot_digits(X_back[:10])\n",
    "plt.subplot(212)\n",
    "plot_digits(X_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 1\n",
    "> **Compare this last result and the time to learn the coding with that of PCA. Which are the pros and cons of each method?**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 2\n",
    "> **Provide some example in your research topic where dimensionality reduction should be used, and explain which of two previous methods you would use.**\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1163px",
    "left": "1185.6px",
    "top": "111.133px",
    "width": "344.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
