{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING FOR RESEARCHERS\n",
    "\n",
    "# Notebook 2. Advanced models for supervised learning\n",
    "\n",
    "\n",
    "\n",
    "This notebook continues the review of supervised machine learning methods for *regression* and *classification*. \n",
    "\n",
    "The following contents are covered: \n",
    "\n",
    "<ol>\n",
    "    <li> Decision Trees </li>\n",
    "    <li> Ensemble methods: Random Forest </li>\n",
    "    <li> Kernel methods applied to linear regression </li>\n",
    "    <li> Support Vector Machines </li>\n",
    "</ol>\n",
    "\n",
    "It is highly recommended that this notebook is read and run after a first reading of the theory and in parallel with the slides available in AV. \n",
    "Note also that it is not required to develop any code. All examples are totally implemented, and therefore these notebooks have to be regarded as demonstrative material. The goal is understanding the operation of the algorithms. The notebook contains several questions that have be to submitted through AV. \n",
    "\n",
    "As with Notebook 1, the codes used for loading and plotting the Iris and MNIST data sets have been taken from <a href=https://github.com/ageron/handson-ml2>Geron (Github site)</a>, as well as the functions to plot the classification boundaries. Please, consult the textbook for reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:35:20.585781Z",
     "start_time": "2020-02-28T18:35:20.580980Z"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision tree (DT) are **non-linear algorithms for classification and regression**. They are considered white-box algorithms since it is easy for a human to understand why a given decision has been made, in contrast to black-box methods like neural networks. \n",
    "\n",
    "In DT, an **algorithm selects at each step which feature** (of those not considered before) **classifies better the training examples**. Then, for each possible value of that feature, a new node is created in the tree, and the process continues until no more features left.\n",
    "\n",
    "A common way to select the best feature at each step is to use the\n",
    "**information gain** concept.  Given a random variable which takes $K$\n",
    "possible outputs $\\{o_k\\}$, for $k$=$1,\\ldots,K$, its **entropy** is given\n",
    "by:\n",
    "\n",
    "\\begin{equation}\n",
    "H = -\\sum_{k=1}^K p_k \\log_2 p_k\n",
    "\\end{equation}\n",
    "\n",
    "Given the labeled data set $\\pmb{X}$, $\\pmb{t}$ its entropy $H(\\pmb{X},\n",
    "\\pmb{t})$ can be estimated as $\\widehat{p_k}$ = $N_k/N$ (that is, the ratio of each **label** in the training set).\n",
    "\n",
    "Besides, the $m^{\\text{th}}$ feature takes values on a set of discrete values\n",
    "$\\pmb{v}_m$ = $\\{v_1,\\ldots,v_K\\}$, where $K$ depends on the specific feature $m$. Let $\\pmb{X},\\pmb{t}|_{(\\pmb{x})_m=v}$ denote the data set comprising only the instances where the $m^{\\text{th}}$ feature takes value $v$. The **information gain** associated to this feature is:\n",
    "\n",
    "\\begin{equation}\n",
    "I|_{(\\pmb{x})_m} = H(\\pmb{X}, \\pmb{t}) - \\sum\\limits_{\\forall v \\in \\pmb{v}_m} p((\\pmb{x})_m=v) H(\\pmb{X}, \\pmb{t}|_{(\\pmb{x})_m=v})\n",
    "\\end{equation}\n",
    "\n",
    "where $p((\\pmb{x})_m=v)$ is estimated as $\\#(\\pmb{X},\\pmb{t}|_{(\\pmb{x})_m=v})/\\#(\\pmb{X},\\pmb{t})$, where $\\#S$ denotes the number of instances in set $S$.\n",
    "\n",
    "This DT algorithm selects at each step **the feature that maximizes the\n",
    "information gain**. In other words, the feature whose knowledge reduces the most\n",
    "uncertainty about the final output.\n",
    "\n",
    "To exemplify this operation we will load an example data set and see how the algorithm behaves step by step. Later, we present the sklearn classes to easily build DTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Teaching Assistant Evaluation (TAE) data set\n",
    "\n",
    "This data set consists of evaluations of teaching performance over three\n",
    "   regular semesters and two summer semesters of 151 teaching assistant\n",
    "   (TA) assignments at the Statistics Department of the University of\n",
    "   Wisconsin-Madison. The scores were divided into 3 roughly equal-sized\n",
    "   categories (\"low\", \"medium\", and \"high\") to form the class variable.\n",
    "\n",
    "Number of Instances: 151\n",
    "Number of Attributes: 6 \n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "   0. Whether of not the TA is a native English speaker (binary)\n",
    "      1=English speaker, 2=non-English speaker\n",
    "   1. Course instructor (categorical, 25 categories)\n",
    "   2. Course (categorical, 26 categories)\n",
    "   3. Summer or regular semester (binary) 1=Summer, 2=Regular\n",
    "   4. Class size (numerical)\n",
    "   5. (**LABEL**): Class attribute (categorical) 1=Low, 2=Medium, 3=High\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:40.951712Z",
     "start_time": "2020-04-11T11:45:39.913166Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/tae/tae.data'\n",
    "myfile = requests.get(url)\n",
    "open('datasets/tae.data', 'wb').write(myfile.content)\n",
    "data = np.genfromtxt('datasets/tae.data', delimiter=',')\n",
    "X_TAE,t_TAE = data[:,0:-1], data[:,-1]\n",
    "X,t = X_TAE,t_TAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:41.225801Z",
     "start_time": "2020-04-11T11:45:40.953357Z"
    }
   },
   "outputs": [],
   "source": [
    "# For data plotting\n",
    "import os\n",
    "\n",
    "# Required to use matplotlib inside the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:41.337136Z",
     "start_time": "2020-04-11T11:45:41.227605Z"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(t):\n",
    "    H = 0\n",
    "    values = np.unique(t)\n",
    "    for value in values.tolist():\n",
    "        p = np.sum(t==value)/t.shape[0]\n",
    "        H -= p*np.log2(p) if p>0 else 0\n",
    "    return H\n",
    "        \n",
    "def selectfeat(X,t):\n",
    "    # Search the feature (attribute) which maximizes the informacion gain\n",
    "    infgain=np.zeros([X.shape[1],])\n",
    "    \n",
    "    for idx in range(X.shape[1]):\n",
    "        values = np.unique(X[:,idx])\n",
    "        infgain[idx] = entropy(t)\n",
    "        for value in values.tolist():\n",
    "            infgain[idx] -= np.sum(X[:,idx]==value)/X.shape[0]*entropy(t[X[:,idx]==value])\n",
    "    return np.argmax(infgain), infgain\n",
    "\n",
    "def DT(X,t,level=0):\n",
    "    attr, infgain = selectfeat(X,t)\n",
    "    if infgain[attr]==0: \n",
    "        return f'is LEAF-> PREDICT {np.argmax(np.bincount(t.astype(int)))}'\n",
    "        \n",
    "    tree = f'\\n'+f'\\t'*level\n",
    "    tree += f'Attribute {attr} selected. Its information gain is {infgain[attr]:.03f}\\n'\n",
    "    values = np.unique(X[:,attr])\n",
    "    for value in values.tolist():\n",
    "        tree += f'\\t'*(level+1)+f'Node created for value {value} in attributte {attr} '\n",
    "        tree += DT(X[X[:,attr]==value], t[X[:,attr]==value], level+1)+'\\n'\n",
    "    return tree\n",
    "        \n",
    "\n",
    "print(DT(X,t))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most relevant attribute seems the class size (4th attribute). However, we can see some **overfitting since this attribute is not categorical, but quantitative**. Indeed, we can select some categorical levels for it. For example, from 0-10 students (small), from 11-30 (mid), and from 31-66 (big), and redo the tree:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:41.431691Z",
     "start_time": "2020-04-11T11:45:41.338761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data is duplicated so this cell can be executed several times with consistent results\n",
    "Xaux = X.copy()\n",
    "Xaux[X[:,4]<11,4]=0\n",
    "Xaux[(11<=X[:,4]) & (X[:,4]<31),4]=1\n",
    "Xaux[(31<=X[:,4]) & (X[:,4]<11),4]=2\n",
    "\n",
    "print(DT(Xaux,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dominant attribute is the particular course to be teach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Playtennis data set\n",
    "\n",
    "We will check how the DT is build for the Playtennis example shown in the Unit 1 slides. The following codification will be used:\n",
    "\n",
    "Attributes: \n",
    "0. Outlook: Sunny(0), Overcast(1), Rain(2)\n",
    "1. Temperature: Hot(0), Mild(1), Cool(2)\n",
    "2. Humidity: High(0), Normal(1)\n",
    "3. Wind: Weak(0), Strong(1)\n",
    "4. (**LABEL**) Playtennis: No(0), Yes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:41.446928Z",
     "start_time": "2020-04-11T11:45:41.433085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's hard-code the data set\n",
    "\n",
    "X = np.array([[0,0,0,0],\n",
    "    [0,0,0,1],\n",
    "    [1,0,0,0],\n",
    "    [2,1,0,0],\n",
    "    [2,2,1,0],\n",
    "    [2,2,1,1],\n",
    "    [1,2,1,1],\n",
    "    [0,1,0,0],\n",
    "    [0,2,1,0],\n",
    "    [2,1,1,0],\n",
    "    [0,1,1,1],\n",
    "    [1,1,0,1],\n",
    "    [1,0,1,0],\n",
    "    [2,1,0,1]])\n",
    "\n",
    "t = np.array([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0])\n",
    "print(DT(X,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It can be seen that the grown tree matches the one shown in the slides' example. Besides the information gain for the outlook attribute matches the one in the computations performed also in the slides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 1\n",
    "> **Observing the resulting DT, provide the class prediction for \"Rain, Mild, High, Weak\" instance, and indicate which nodes of the tree are traversed (i.e., which attributes are taken into account to make the decision).**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT with sklearn\n",
    "\n",
    "Next, we will examine how to implement DT directly with sklearn with the class **DecisionTreeClassifier**. In this case results are different because, even using entropy as the criterion to build the tree, the algortithm used by sklearn is CART, and the one presented in the theory is ID3. Anyway, the tree formation and the general ideas remain. You can consult [this link](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) for a short discussion about these algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:41.780737Z",
     "start_time": "2020-04-11T11:45:41.448202Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# To use entropy select criterion='entropy', by default it uses gini impurity\n",
    "clf = DecisionTreeClassifier(criterion='gini',random_state=0).fit(X,t)\n",
    "print(f'Prediction for rain, mild day with high pressure and weak wind is {\"No\" if (clf.predict(np.array([2,1,0,0]).reshape(1,-1))==0) else \"Yes\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAE data set with sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:41.806841Z",
     "start_time": "2020-04-11T11:45:41.782192Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "X,t = X_TAE,t_TAE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# To use entropy select criterion='entropy', by default it uses gini impurity\n",
    "clf = DecisionTreeClassifier(criterion='gini',random_state=0)\n",
    "print(cross_val_score(clf, X, t, cv=5)) # Does CV runs with separated validataion and training sets. \n",
    "                                 # Output is the accuracy over the validation set\n",
    "\n",
    "clf_entropy = DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "print(cross_val_score(clf_entropy, X, t, cv=5)) # Does CV runs with separated validataion and training sets. \n",
    "                                 # Output is the accuracy over the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sperical vs. Torus data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:41.821236Z",
     "start_time": "2020-04-11T11:45:41.809060Z"
    }
   },
   "outputs": [],
   "source": [
    "## Function to plot decision boundaries\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris virginica\")\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:42.421655Z",
     "start_time": "2020-04-11T11:45:41.823087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create data set\n",
    "#\n",
    "np.random.seed(1)\n",
    "r = np.random.rand(50, 1) #0...1\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_0 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "r = 0.9 + np.random.rand(50, 1) #0.9...1.9\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_1 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "X = np.r_[X_0, X_1]\n",
    "t = np.r_[np.zeros([50,1]),np.ones([50,1])].ravel()\n",
    "\n",
    "# Create DT \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0).fit(X,t)\n",
    "\n",
    "# Show resulting boundaries\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(clf, X, t, axes=[-2,2,-2,2], iris=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wedge data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:42.637309Z",
     "start_time": "2020-04-11T11:45:42.423118Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X0 = 10*np.random.rand(100, 1) \n",
    "X1 = 10*np.random.rand(100, 1) \n",
    "X = np.c_[X0,X1]\n",
    "\n",
    "t = np.logical_and(X[:,0]>X[:,1]**(1/1.5),X[:,0]<X[:,1]**(1.5)).ravel()\n",
    "clf = DecisionTreeClassifier(random_state=0).fit(X,t)\n",
    "\n",
    "# Show resulting boundaries\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(clf, X, t, axes=[0,10,0,10], iris=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the boundaries are formed by a combination of squared shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:35:46.618199Z",
     "start_time": "2020-02-28T18:35:46.613465Z"
    }
   },
   "source": [
    "## Ensemble methods: Random Forest\n",
    "\n",
    "An **ensemble** is a learning structure which **combines many classifiers and makes predictions as the majority vote** of all classifiers. A **random forest is an ensemble formed by decision trees** trained with the different training data (selected at random from the total training data). This method is **non-linear** and they can be used both for regression and classification. \n",
    "\n",
    "First, we will show the results for the two previous examples and next for the California House data set already used in the Notebook 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shperical vs. Torus data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:43.098620Z",
     "start_time": "2020-04-11T11:45:42.638645Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "r = np.random.rand(50, 1) #0...1\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_0 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "r = 0.9 + np.random.rand(50, 1) #0.9...1.9\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_1 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "X = np.r_[X_0, X_1]\n",
    "t = np.r_[np.zeros([50,1]),np.ones([50,1])].ravel()\n",
    "\n",
    "# Create Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0).fit(X,t)\n",
    "\n",
    "# Show resulting boundaries\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(clf, X, t, axes=[-2,2,-2,2], iris=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wedge data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:43.525208Z",
     "start_time": "2020-04-11T11:45:43.099963Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X0 = 10*np.random.rand(100, 1) \n",
    "X1 = 10*np.random.rand(100, 1) \n",
    "X = np.c_[X0,X1]\n",
    "\n",
    "t = np.logical_and(X[:,0]>X[:,1]**(1/1.5),X[:,0]<X[:,1]**(1.5)).ravel()\n",
    "clf = RandomForestClassifier(random_state=0).fit(X,t)\n",
    "\n",
    "# Show resulting boundaries\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(clf, X, t, axes=[0,10,0,10], iris=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 2\n",
    "> **Explain why the Random Forest may have non-squared decision boundaries.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The California House data set\n",
    "\n",
    "Decision trees and Random forest can be also used in **regression contexts**. We will exemplify their use for the example of the California House data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:44.434716Z",
     "start_time": "2020-04-11T11:45:43.526633Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:45.668706Z",
     "start_time": "2020-04-11T11:45:44.436219Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Notebook 1, the raw table is prepared for its use in the regression model. For that the following steps are done:\n",
    "\n",
    "<ol>\n",
    "    <li> Computation of representative features (e.g. rooms_per_household)\n",
    "    <li> Separation of targets and features\n",
    "    <li> Substitution of missing features (e.g. number of rooms) for the median value of the column\n",
    "    <li> Substitution of categorical variables (e.g. ocean_proximity) for a 1-K encoding\n",
    "    <li> Separation of training and test sets\n",
    "</ol>\n",
    "\n",
    "You can skip most of this step since it involves dataframes and ndarrays manipulation. The significant part is that at the end result is divided in training and testing sets, and targets are provided in separated vectors as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:45:45.745085Z",
     "start_time": "2020-04-11T11:45:45.670112Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "housing = load_housing_data() # Since we perform destructive change, we reload the data\n",
    "targets = housing[\"median_house_value\"].copy()\n",
    "housing = housing.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "X_train_CALHOUSE, X_test_CALHOUSE, t_train_CALHOUSE, t_test_CALHOUSE = train_test_split(housing_prepared, targets, test_size=0.2) # Reserve 20% of instances for testing purposes\n",
    "X_train, X_test, t_train, t_test = X_train_CALHOUSE, X_test_CALHOUSE, t_train_CALHOUSE, t_test_CALHOUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:01.266421Z",
     "start_time": "2020-04-11T11:45:45.746621Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = RandomForestRegressor(random_state=0).fit(X_train,t_train)\n",
    "\n",
    "y_train = reg.predict(X_train)\n",
    "y_test = reg.predict(X_test)\n",
    "rmse_train = np.sqrt(mean_squared_error(t_train, y_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(t_test, y_test))\n",
    "\n",
    "print(f'The score (rmse) in the training set is {rmse_train:.00f}, with accuracy of {reg.score(X_train,t_train)*100:.02f}%')\n",
    "print(f'The score (rmse) in the test set is {rmse_test:.00f}, with accuracy of {reg.score(X_test,t_test)*100:.02f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results is **much better** that the one achieved with a regularized linear regression!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Kernel Linear Regression\n",
    "\n",
    "In Notebook 1 we have seen that the linear regression model assumes a parametric hypothesis of the form:\n",
    "\n",
    "  \\begin{equation}\n",
    "  y(\\pmb{x}, \\pmb{w}) = \\sum_{j=0}^{M-1} w_j \\phi_j(\\pmb{x}) = \\pmb{w}^T \\pmb{\\phi}(\\pmb{x})\n",
    "  \\end{equation}\n",
    "\n",
    "The parameters $\\pmb{w}$ can be determined by minimizing the **regularized squared error**. For the particular case of Ridge regression ($q$=2) we got the loss:\n",
    "\n",
    "\\begin{equation}\n",
    " \\widetilde{J}(\\pmb{w})  = \\frac{1}{2} \\sum_{n=1}^N (\\pmb{w}^T \\pmb{\\phi}(\\pmb{x}_n)- t_n)^2 + \\frac{\\lambda}{2} \\pmb{w}^T\\pmb{w}\n",
    "  \\end{equation}\n",
    "\n",
    "Leading to the normal equations: \n",
    "\n",
    "\\begin{equation}\n",
    "  \\pmb{w}^* = (\\lambda I + \\pmb{\\Phi}^T \\pmb{\\Phi})^{-1} \\pmb{\\Phi}^T \\pmb{t}\n",
    "\\end{equation}\n",
    "\n",
    "which can be recast as:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\pmb{w}^* = \\pmb{\\Phi}^T \\pmb{a}\n",
    "  \\end{equation*}\n",
    "\n",
    "  where $\\pmb{a}$ is the vector defined by the set $\\{a_n = -\\frac{1}{\\lambda} (\\pmb{w}^T\\pmb{\\phi}(\\pmb{x}_n) - t_n)\\}$. \n",
    "  \n",
    "By using $\\pmb{a}$ as the optimization problem variable (instead of $\\pmb{w}$) we got the dual representation form for the loss:\n",
    "\n",
    "  \\begin{equation}\n",
    "  \\tilde{J}(\\pmb{a}) = \\frac{1}{2} \\pmb{a}^T \\pmb{K}\\pmb{K}\\pmb{a} - \\pmb{a}^T\\pmb{K}\\pmb{t}+\\frac{1}{2} \\pmb{t}^T \\pmb{t} + \\frac{\\lambda}{2}\n",
    "  \\pmb{a}^T\\pmb{K}\\pmb{a}\n",
    "  \\end{equation}\n",
    "\n",
    "where $\\pmb{K}$ is the [**Gram matrix**](https://en.wikipedia.org/wiki/Gramian_matrix): $K$ = $\\pmb{\\Phi} \\pmb{\\Phi}^T$. That is, $(\\pmb{K})_{nm}$ = $\\pmb{\\phi}(\\pmb{x}_n)^T \\pmb{\\phi}(\\pmb{x}_m)$ = $k(\\pmb{x}_n,\\pmb{x}_m)$.\n",
    "\n",
    "And the predictions are computed in the dual form as:\n",
    "\n",
    " \\begin{equation}\n",
    "  y(\\pmb{x},\\pmb{w}) = \\pmb{w}^T\\pmb{\\phi}(\\pmb{x}) = \\pmb{a}^T\\pmb{\\Phi}\\pmb{\\phi}(\\pmb{x}) =\n",
    "  \\pmb{k}(\\pmb{x})^T(\\pmb{K}+\\lambda\\pmb{I})^{-1}\\pmb{t}\n",
    "  \\label{DUALPRED}\n",
    "  \\end{equation}\n",
    "\n",
    "  Note that previous equation doesn't require the use of parameters the $\\pmb{w}$, but\n",
    "instead we have recast our **parametric-based model** problem to an **instance-based** method (prediction is provided in terms of the *kernel evaluations of point $\\pmb{x}$ against each training point*). In addition, data conversion to/from the feature space can be avoided using the **kernel\n",
    "  trick**, as will be illustrated below.\n",
    "  \n",
    "First, the data set used in the Notebook 1 will be created:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:01.448352Z",
     "start_time": "2020-04-11T11:46:01.267941Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Libraries required\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Required to use matplotlib inside the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# We set a fixed seed to get always the same results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Data set generation\n",
    "X = np.linspace(0,1,num=10)\n",
    "X = np.array([X]).T # Since X is single-dimensional, we have to convert it to a matrix to transpose it \n",
    "t = np.sin(2*np.pi*X) + 0.1*np.random.randn(10, 1)\n",
    "xfull = np.linspace(0,1,num=100);\n",
    "xfull = np.array([xfull]).T\n",
    "treal = np.sin(2*np.pi*xfull);\n",
    "\n",
    "# Data set plotting\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$t$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear kernel\n",
    "\n",
    "The simplest approximation is working with the *linear kernel*: $k(\\pmb{x},\\pmb{x}')$ = $\\pmb{x}^T \\pmb{x}'$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:01.453397Z",
     "start_time": "2020-04-11T11:46:01.449700Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, t, x, L, kernel): # Now prediction is done with data set X,t as argument. No \"fit\" necessary\n",
    "                                 # kernel can be any function created latter\n",
    "    K = kernel(X,X)\n",
    "    y = kernel(X,x).T.dot(np.linalg.inv(K+L*np.eye(K.shape[0]))).dot(t)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how it works in the toy data set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:01.634981Z",
     "start_time": "2020-04-11T11:46:01.454696Z"
    }
   },
   "outputs": [],
   "source": [
    "def linK(X,XX): # Both arguments are matrices (one instance per row)\n",
    "    return X.dot(XX.T)\n",
    "\n",
    "y = predict(X, t, xfull, L=np.exp(-10), kernel=linK)\n",
    "\n",
    "plt.plot(xfull, y, \"r-\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial kernel with 2-Degree terms\n",
    "\n",
    "Now, let's consider  $k(\\pmb{x},\\pmb{x}')$ = $(\\pmb{x}^T \\pmb{x}')^2$.\n",
    "\n",
    "As shown in the slides, this is equivalent to use the feature space $\\pmb{\\phi}(\\pmb{x})$=$(x_1^2,\\sqrt{2}x_1 x_2,\n",
    "  x_2^2)^T$, but **without explicit conversions to or from it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:01.796815Z",
     "start_time": "2020-04-11T11:46:01.636353Z"
    }
   },
   "outputs": [],
   "source": [
    "def pol2K(X,XX): # Both arguments are matrices (one instance per row)\n",
    "    return (X.dot(XX.T))**2\n",
    "\n",
    "y = predict(X, t, xfull, L=np.exp(-8), kernel=pol2K)\n",
    "\n",
    "plt.plot(xfull, y, \"r-\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better! Let's try with some $\\infty$-dimensional kernels (yes, this is possible and easy with the kernel trick!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF kernel\n",
    "\n",
    "The RBF kernel is defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "k(\\pmb{x},\\pmb{x}') = \\exp{\\big(\\frac{-\\|\\pmb{x}-\\pmb{x}'\\|^2}{2\\sigma^2}\\big)}\n",
    "\\end{equation}\n",
    "\n",
    "The associated feature space has infinite dimensionality. Let's implement it. For that, note that: $\\|\\pmb{x}-\\pmb{x}'\\|^2$ = $\\pmb{x}^T\\pmb{x} + (\\pmb{x}')^T\\pmb{x}' - 2\\pmb{x}^T\\pmb{x}'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:01.960551Z",
     "start_time": "2020-04-11T11:46:01.798197Z"
    }
   },
   "outputs": [],
   "source": [
    "SIGMA = np.exp(0)\n",
    "\n",
    "def gaussK(X,XX): # Both arguments are matrices (one instance per row)\n",
    "    global SIGMA\n",
    "    aux = np.add(X.dot(X.T).diagonal().T.reshape([-1,1]),XX.dot(XX.T).diagonal().reshape([1,-1]))\n",
    "    return np.exp(-(aux-2*X.dot(XX.T))/(2*SIGMA))\n",
    "\n",
    "y = predict(X, t, xfull, L=np.exp(-8), kernel=gaussK)\n",
    "\n",
    "plt.plot(xfull, y, \"r-\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels with sklearn\n",
    "\n",
    "Next, we will discuss how to use kernels directly with the sklearn library. The class which provides it is called **KernelRidge** and its use its quite straighforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:02.126429Z",
     "start_time": "2020-04-11T11:46:01.961999Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "ridge = KernelRidge(alpha=np.exp(-10), kernel='linear')\n",
    "ridge.fit(X, t) # Just precomputes kernel(X,X) which will be needed in predict\n",
    "y = ridge.predict(xfull) \n",
    "\n",
    "plt.plot(xfull, y, \"r-\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try other kind of kernels (check [here the list of available kernels](https://scikit-learn.org/stable/modules/metrics.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:02.889309Z",
     "start_time": "2020-04-11T11:46:02.129439Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "ridge1 = KernelRidge(alpha=np.exp(-10), kernel='rbf') # k(x,x') = exp(-gamma*||x-x'||^2)\n",
    "ridge2 = KernelRidge(alpha=np.exp(-10), kernel='poly', degree=20) \n",
    "ridge3 = KernelRidge(alpha=np.exp(-10), kernel='poly', degree=5) \n",
    "ridge4 = KernelRidge(alpha=np.exp(-10), kernel='sigmoid') # k(x,x') = tanh(gamma*x^T x' + c_0)\n",
    "ridge1.fit(X, t) # Just precomputes kernel(X,X) which will be needed in predict\n",
    "ridge2.fit(X, t)\n",
    "ridge3.fit(X, t) \n",
    "ridge4.fit(X, t)\n",
    "y1 = ridge1.predict(xfull)\n",
    "y2 = ridge2.predict(xfull)\n",
    "y3 = ridge3.predict(xfull)\n",
    "y4 = ridge3.predict(xfull)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,8))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0,0].set_title(f'RBF kernel')\n",
    "ax[0,0].plot(xfull, y1, \"r-\")\n",
    "ax[0,1].set_title(f'Poly kernel, degree=20')\n",
    "ax[0,1].plot(xfull, y2, \"m-\")\n",
    "ax[1,0].set_title(f'Poly kernel, degree=5')\n",
    "ax[1,0].plot(xfull, y3, \"k-\")\n",
    "ax[1,1].set_title(f'Sigmoidal kernel')\n",
    "ax[1,1].plot(xfull, y4, \"y-\")\n",
    "\n",
    "ax[0,0].plot(xfull, treal, \"g-\")\n",
    "ax[0,1].plot(xfull, treal, \"g-\")\n",
    "ax[1,0].plot(xfull, treal, \"g-\")\n",
    "ax[1,1].plot(xfull, treal, \"g-\")\n",
    "\n",
    "ax[0,0].plot(X, t, \"b.\")\n",
    "ax[0,1].plot(X, t, \"b.\")\n",
    "ax[1,0].plot(X, t, \"b.\")\n",
    "ax[1,1].plot(X, t, \"b.\")\n",
    "\n",
    "ax[0,0].axis([0, 1, -1.5, 1.5])\n",
    "ax[0,1].axis([0, 1, -1.5, 1.5])\n",
    "ax[1,0].axis([0, 1, -1.5, 1.5])\n",
    "ax[1,1].axis([0, 1, -1.5, 1.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different kernels have different hyper-parameters. In order to find the best ones and the regularization coefficient ($\\lambda$) we can use cross-validation with sklearn too. In this case there is no the equivalent to **LinearRegressionCV** in the library so it's necessary to implement it. The **GridSearchCV** can be used for this purpose (and in any other cross-validation search):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:05.976912Z",
     "start_time": "2020-04-11T11:46:02.891837Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GridSearchCV(KernelRidge(kernel='poly'), \n",
    "             cv=5, \n",
    "             param_grid={\"alpha\": np.exp(np.arange(-8,-6,0.1)), \"degree\": np.arange(1, 20, 1)})\n",
    "model.fit(X, t)\n",
    "y = model.predict(xfull)\n",
    "print(f'The best model is: {model.best_params_}')\n",
    "\n",
    "plt.plot(xfull, y, \"r-\")\n",
    "plt.plot(xfull, treal, \"g-\")\n",
    "plt.plot(X, t, \"b.\")\n",
    "plt.axis([0, 1, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 3\n",
    "> **Have a look at the predict function implemented above. Is there any way to speed up its operation by  precomputing some of the computations? In other words, which operations do not depend on the new data instance and can be moved outside predict?**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "SVMs are **linear** classification models of **maximum-margin** for which the dual form (instance-based) is **sparse**. In the slides we have seen that the primal form to find the boundary weights $\\pmb{w}$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min\\limits_{\\pmb{w},b} \\frac{1}{2} \\|\\pmb{w}\\|^2\n",
    "\\label{PRIMAL}\n",
    "\\end{equation}\n",
    "\n",
    "subject to $t_n [\\pmb{w}^T \\pmb{\\phi}(\\pmb{x}_n) + b] \\ge 1$. The prediction for a new point $\\pmb{x}$ corresponds to $y(\\pmb{x}) = \\text{sign}(\\pmb{w}^T \\pmb{\\phi}(\\pmb{x}) + b)$.\n",
    "\n",
    "Whereas, the dual form is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max\\limits_{\\pmb{a}} \\widetilde{L}(\\pmb{a}) = \\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_na_mt_nt_mk(\\pmb{x}_n,  \\pmb{x}_m)\n",
    "\\label{DUAL}\n",
    "\\end{equation}\n",
    "\n",
    "subject to $a_n\\ge0$, for $n=1,\\ldots,N$, and $\\sum_{n=1}^N a_nt_n = 0$.\n",
    "\n",
    "\n",
    "Both the primal and dual forms are [quadratic programming problems](https://en.wikipedia.org/wiki/Quadratic_programming), thus convex, and since the region\n",
    "determined by the constraints is convex, there is a single\n",
    "optimal.\n",
    "\n",
    "Solving quadratic problems has order cubic order on the number of variables.\n",
    "Thus, the primal is $O(M^3)$, and the dual is $O(N^3)$. Therefore **the dual is\n",
    "advantageous for high-dimensional feature spaces**. \n",
    "\n",
    "Besides, the kernel trick allows the direct computation of kernels in the input space. Moreover, since in the solution $a_n \\ne 0$ only for a subset of the $N$ training points (called **suppor vectors (SV)**, the prediction **does not require to evaluate the kernel versus all training set instances, but only against those in SV**, leading to:\n",
    "\n",
    "\\begin{equation*}\n",
    "y(\\pmb{x}) = \\text{sign}(\\sum_{x_n\\text{ is SV}} a_nt_nk(\\pmb{x},\\pmb{x_n}) + b)\n",
    "\\end{equation*}\n",
    "\n",
    "It is important to note that, **unlike linear regression with kernels, in this case the model must be trained to find which data are support vectors**. Then, **calculating the predictions requires evaluating the kernel for the point to be predicted against those support vectors** obtained during training.\n",
    "\n",
    "Besides, SVMs also support **soft-margin** solutions, that is allowing\n",
    "some points to lie in the margin, or even being in the wrong side of the separation boundary. For that points some form of penalty is included in the primal form, leading to slightly modified dual forms.\n",
    "\n",
    "We are going how to use SVM with the implementation available in sklearn with the class **sklearn.svm.SVC** (SVC stands from Support Vector **Classification**, since other SVM variations are available for regression and even unsupervised learning). For large data sets (case of MNIST) the alternative **LinearSVC** class can be used.\n",
    "\n",
    "In the following examples we will show how to apply this method for different data sets. Where possible, the instances which are support vectors are highlighted. Note also that the SVC implementation considers a regularization coefficient $1/C$ for $\\|\\pmb{w}\\|^2$ in the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearly separable data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:06.166334Z",
     "start_time": "2020-04-11T11:46:05.978327Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "r = 0.3*np.random.rand(50, 1) #0...1\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_0 = np.c_[1+r*3*np.cos(a), 5*r*np.sin(a)]\n",
    "\n",
    "r = 0.3*np.random.rand(50, 1) #0.9...1.9\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_1 = np.c_[-1+r*2*np.cos(a), 5*r*np.sin(a)]\n",
    "\n",
    "X = np.r_[X_0, X_1]\n",
    "t = np.r_[np.zeros([50,1]),np.ones([50,1])].ravel()\n",
    "\n",
    "# Plot data set\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(X[t==0, 0], X[t==0, 1], \"bs\")\n",
    "plt.plot(X[t==1, 0], X[t==1, 1], \"g^\")\n",
    "\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:06.174101Z",
     "start_time": "2020-04-11T11:46:06.167732Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
    "    plt.plot(x0, gutter_down, \"k--\", linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:06.405629Z",
     "start_time": "2020-04-11T11:46:06.175444Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear',C=10) # The highest C, the more flexible the model\n",
    "clf.fit(X, t)\n",
    "print(f'The score (accurary) in the training set is {clf.score(X,t)*100:.02f}%')\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "plot_svc_decision_boundary(clf, -2, 2)\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In red circles the SV are highlighted. Note that determining the class of a new point $\\pmb{x}$ only requires kernel evaluation against these 6 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:52:16.916475Z",
     "start_time": "2020-02-28T18:52:16.914180Z"
    }
   },
   "source": [
    "### Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:06.638070Z",
     "start_time": "2020-04-11T11:46:06.407002Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "# print(iris.DESCR) # Uncomment to show infor about the data set\n",
    "\n",
    "# Let's create the feature and the target set\n",
    "X = iris[\"data\"][:, (2, 3)]  # Petal length and width\n",
    "t = (iris[\"target\"] == 2).astype(np.int)\n",
    "t = t.reshape([X.shape[0],1])\n",
    "\n",
    "# Since only two features are used, we can plot the data set\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "\n",
    "plt.text(3.5, 1.5, \"Non Iris-Virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris-Virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:06.851890Z",
     "start_time": "2020-04-11T11:46:06.639447Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, t.ravel())\n",
    "print(f'The score (accurary) in the training set is {clf.score(X,t)*100:.02f}%')\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "plot_svc_decision_boundary(clf, 3, 7)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, data is not linearly separable, and some points are allowed in the wrong side of the decision boundary. Note that more points are SVs in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spherical vs. Torus data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:07.041008Z",
     "start_time": "2020-04-11T11:46:06.853380Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "r = np.random.rand(50, 1) #0...1\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_0 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "r = 0.9 + np.random.rand(50, 1) #0.9...1.9\n",
    "a = 2*np.pi*np.random.rand(50,1) #0..2pi\n",
    "X_1 = np.c_[r*np.cos(a), r*np.sin(a)]\n",
    "\n",
    "X = np.r_[X_0, X_1]\n",
    "t = np.r_[np.zeros([50,1]),np.ones([50,1])].ravel()\n",
    "\n",
    "# Plot data set\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(X[t==0, 0], X[t==0, 1], \"bs\")\n",
    "plt.plot(X[t==1, 0], X[t==1, 1], \"g^\")\n",
    "\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:07.048384Z",
     "start_time": "2020-04-11T11:46:07.042399Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:07.326396Z",
     "start_time": "2020-04-11T11:46:07.049690Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='poly',degree=2)\n",
    "clf.fit(X, t)\n",
    "print(f'The score (accurary) in the training set is {clf.score(X,t)*100:.02f}%')\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X[t.reshape(X.shape[0],)==0, 0], X[t.reshape(X.shape[0],)==0, 1], \"bs\")\n",
    "plt.plot(X[t.reshape(X.shape[0],)==1, 0], X[t.reshape(X.shape[0],)==1, 1], \"g^\")\n",
    "plot_predictions(clf, [-2, 2, -2, 2])\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:30.021274Z",
     "start_time": "2020-04-11T11:46:07.327736Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "# print(mnist.DESCR) # Uncomment to show description\n",
    "X, t = mnist[\"data\"], mnist[\"target\"] # N = 70000, D = 784 \n",
    "X_train, X_test, t_train, t_test = X[:60000], X[60000:], t[:60000], t[60000:]\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:30.159175Z",
     "start_time": "2020-04-11T11:46:30.022839Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.plasma, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:red\"> This fitting requires a high-end computer (about 20 minutes in a machine with 12 cores and 64G of RAM). To execute and save classifier to disk uncomment the following lines. A precomputed model is available in AV. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:30.162659Z",
     "start_time": "2020-04-11T11:46:30.160630Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.svm import LinearSVC\n",
    "#clf = LinearSVC(max_iter=1000000) # Multiclass with one-vs-rest strategy\n",
    "#clf.fit(X_train, t_train)\n",
    "#clf = SVC(kernel='rbf')\n",
    "#clf.fit(X_train, t_train)\n",
    "\n",
    "#from joblib import dump\n",
    "#dump(clf, 'SVM-MNIST-RBF.joblib')  # SAVE TO DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:46:30.231086Z",
     "start_time": "2020-04-11T11:46:30.164041Z"
    }
   },
   "outputs": [],
   "source": [
    "# LOAD PRECOMPUTED SVM CLASSIFIER FROM DISK\n",
    "# The file SVM-MNIST.joblib is available in AV\n",
    "# Download and save in the same folder as the notebook\n",
    "\n",
    "from joblib import load\n",
    "clf = load('SVM-MNIST-RBF.joblib') \n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:red\">Next cell requires approximately 10 minutes to calculate</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T11:59:52.013648Z",
     "start_time": "2020-04-11T11:46:30.232530Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'The score (accurary) in the training set is {clf.score(X_train,t_train)*100:.02f}%')\n",
    "print(f'The score (accuracy) in the test set is {clf.score(X_test,t_test)*100:.02f}%\\n')\n",
    "\n",
    "plot_digits(X_test[:10], images_per_row=10)\n",
    "print('Labels predicted for the 10 first pictures in the test set')\n",
    "print(clf.predict(X_test[:10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-11T11:45:43.045Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = clf.predict(X_test)\n",
    "conf_mx = confusion_matrix(t_test, y_test)\n",
    "print(conf_mx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1163px",
    "left": "1185.6px",
    "top": "111.133px",
    "width": "344.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
